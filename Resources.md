# Deep Learning and Neural Networks Resources

## Books

[The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks, Daniel A. Roberts, Sho Yaida, Boris Hanin, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Principles_of_Deep_Learning_Theory_Roberts_2021.pdf)

[Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory, Arnulf Jentzen, Benno Kuckuck, Phillipe von Wurstemberger, University of Muenster, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Mathematical_Introduction_to_Deep_Learning-Methods_Implementations_and_Theory_Jentzen_2023.pdf)

[Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, Bernhard Schoelkopf, Alexander J. Smola, MIT, 2002](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/scholkopf2002learning_with_kernels.pdf)

[Deep Learning: Foundations and Concepts, Christopher Bishop, Hugh Bishop, Cambridge, UK, 2023, online viewing only](https://issuu.com/cmb321/docs/deep_learning_ebook)

[Understanding Deep Learning, Simon J. Prince, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/UnderstandingDeepLearning_13_10_23_C.pdf)
(book site URL: https://udlbook.github.io/udlbook/)

[Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/deeplearning_latest_edition.pdf)

[Neural Networks - Systematic Introduction, Raul Rojas, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/neuron.pdf)

[Machine Learning - A Probabilistic Perspective, Kevin P. Murphy, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/MachineLearning-AProbabilisticPerspective.pdf)

[Pattern Recognition and Machine Learning, Christopher Bishop, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Bishop-Pattern_Recognition_and_Machine_Learning.pdf)

[Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges, M. Bronstein et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Geometric_Deep_Learning_Grids_Groups_Graphs_Geodesics_and_Gauges_Bronstein_2021.pdf)

[Machine Learning Refined - Foundations, Algorithms, and Applications, Jeremy Watt, Reza Bohrani, Aggelos Katsaggelos, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Machine_Learning_Refined.pdf)

[Foundations of Vector Retrieval, Sebastian Bruch, 2024, arXiv](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Foundations_of_Vector_Retrieval_Bruch_2024.pdf)

[Deep Learning and Computational Physics, Lecture Notes, USC, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Deep_Learning_and_Computational_Physics_LectureNotes_2004.pdf)

[The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning, RY Rubinstein, DP Kroese, 2004](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Cross_Entropy_Method_A_Unified_Approach_Rubinstein_Kroese_2004.pdf)

[Dive into Deep Learning, Interactive deep learning book with code, math, and discussions, Aston Zhang, Zachary Lipton, Mu Li, Alexander Smola, online version](https://d2l.ai/index.html)


## Articles and tutorials
[Learning Representations by Back-propagating Errors, D. Rumelhart, G. Hinton, R. Williams,Nature, 1986](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_representations_by_backpropagating_errors_Rummelhart_Hinton_Williams_1986.pdf)

[Multilayer Feedforward Networks are Universal Approximators, Kurt Hornik, Maxwell Stinchcombe, Halber White, 1989](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MultilayerFeedforwardNetworksAreUniversalApproximatorsHornik89.pdf)

[Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function, M. Leshno, V. Y. Lin, A. Pinkus, S. Schocken, 1993](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MultilayerFeedforwardNetworksWithNonpolynomialActivationFunctionCanApproximateAnyFunctionLeshno1993.pdf)

[ImageNet Classification with Deep Convolutional Neural Networks, Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, NIPS, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf)

[Reducing the Dimensionality of Data with Neural Networks, G. Hinton, R. Salakhutdinov, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ReducingTheDimensionalityOfDataWithNeuralNetsHinton2006.pdf)

[Transforming Auto-encoders, G. Hinton, A. Krizhevsky, S.D. Wang, 2011](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TransformingAutoencodersHinton.pdf)

[On the Importance of Initialization and Momentum in Deep Learning, Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/OnTheImportanceOfInitializationAndMomentumInDeepLearningSutskever13.pdf)

[Distilling the Knowledge in a Neural Network, G. Hinton et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Distilling_the_Knowledge_in_a_Neural_Network_Hinton_2015.pdf)

[Multiple Object Recognition with Visual Attention, Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Multiple_Object_Recognition_with_Visual_Attention_Ba_Mnih_2015.pdf)

[The Forward-Forward Algorithm: Some Preliminary Investigations, Geoffrey Hinton, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/The_Forward-Forward_Algorithm-Some_Preliminary_Investigations_Hinton_2022.pdf)

[Neural Networks and Deep Learning, Michael Nielsen, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NeuralNetworksAndDeepLearningNielsen.pdf)

[Autoencoding Video Frames, Terence Broad, Master Thesis, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoding_Video_Frames_Broad_2016.pdf)

[Deep Learning: Methods and Applications, Li Deng, Dong Yu, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)

[Universal Approximation of an Unknown Mapping and Its Derivatives Using Multilayer Feedforward Networks, Kurt Hornik, Maxwell Stinchcombe, Halbert White, 1990](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/UniversalApproximationOfUnknownMappingHrnik90.pdf)

[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, J. Frankle, M. Carbin MIT CSAIL 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheLotteryTicketHypothesisFindingSparseTrainableNeuralNetworksFrankle2017.pdf)

[A Theory of Transfer Learning with Applications of Active Learning, Liu Yang, Steve Hanneke, Jaime Carbonell, Carnegie Mellon University, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheoryOfTransferLearningWithApplicationsToActiveLearningCMUYang.pdf)

[Learning with Local and Global Consistency, D. Zhou, O. Bousquet, T. Lal, J. Weston, B. Scholkopf, 2003](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-2003-learning-with-local-and-global-consistency-Paper.pdf)

[Bagging, Boosting and Ensemble Methods, Peter Buehlmann, ETH Zurich, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bagging_Boosting_and_Ensemble_Methods_Buhlmann_2010.pdf)

[Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition, Dominik Scherer, Andreas Mueller, Sven Behnke, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/EvalutationOfPoolingOperationsInConvolutionalArchitecturesForObjectRecognition.pdf)

[Learning Deep Architectures for AI, Yoshua Bengio, 2009](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Deep_Architectures_for_AI_Y_Bengio_2009.pdf)

[Learning both Weights and Connections for Efficient Neural Networks, Song Han et al, Stanford U, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/learning-both-weights-and-connections-for-efficient-neural-network-Han-Pool-Tran-Dally-2015.pdf)

[Boosting Algorithms: Regularization, Prediction and Model Fitting, Peter Buehlmann, Torsten Hothorn, 2008](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/BoostingAlgorithmsRegularizationPredictionAndModelFittingBuehlman2007.pdf)

[Least Angle Regression, Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, 2004](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/LeastAngleRegressionEffron2004.pdf)

[Regression Shrinkage and Selection via the Lasso, Robert Tibshirani, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/RegressionShrinkageAndSelectionViaTheLassoTibshirani1996.pdf)

[A Note on Learning Vector Quantization, Virginia de Sa, 1992](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1992-a-note-on-learning-vector-quantization-Paper.pdf)

[Learning Vector Quantization: The Dynamics of Winner-Takes-All Algorithms, M. Biehl et al, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Vector_Quantization-The_Dynamics_of_Winner-Takes-All_Algorithms_Biehl_2006.pdf)

[Six Lectures on Linearized Neural Networks, T. Misiakiewicz, A. Montanari, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/SixLecturesonLinearizedNeuralNetworks_Misiakiewicz_2023.pdf)

[The Modern Mathematics of Deep Learning, J. Berner et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ModernMathematicsOfDeepLearning2021.pdf)

[Introduction To Metric Fixed Point Theory, M.A. Khamsi, 2002](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/IntroductionToMetricFIxedPointTheoryKhamsi.pdf)

[A Global Geometric Framework for Nonlinear Dimensionality Reduction, J. Tenenbaum, et al, 2000](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/GlobalGeometricFrameworkForNonlinearDimensionalityReductionTenenbaum2000.pdf)

[Online Learning and Stochastic Approximations, Leon Bottou, AT&T Labs, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/OnlineAlgorithmsAndStochasticApproximationsBottou1998.pdf)

[A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, Yoav Freund, Robert E Schapire, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/decision-theoretic_generalization_of_online_learning_and_app_to_Boosting_Freund1996.pdf)

[Improving Generalization With Active Learning, David Cohn, Les Atlas, Richard Landner, 1994](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Cohn1994_ImprovingGeneralizationWithActiveLearning.pdf)

[High-Dimensional Dynamics of Generalization Error in Neural Networks, M.S. Advani et al, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/High-dimensional_dynamics_of_generalization_error_in_neural_networks_Advani_2017.pdf)

[Pruning Convolutional Neural Networks for Resource Efficient Inference, P. Molchanov, S. Tyree, T. Karras, T. Aila, J. Kautz, NVIDIA, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/PruningCNNForResourceEfficientInferenceMolchanov2017.pdf)

[What is The State of Neural Network Pruning? D. Blalock, J. Ortiz, J. Frankle, J. Guttag, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/WhatIsTheStateOfNeuralNetworkPruningBlalock2020.pdf)

[Deep Ensembles: A Loss Landascape Perspective, S. Fort et al, Google Research, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Deep_Ensembles-A_Loss_Landscape_Perspective_Fort_GoogleBrain_2020.pdf)

[You Only Look Once: Unified, Real-Time Object Detection, J. Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/YOLO-UnifiedRealTimeObjectDetection.pdf)

[Employing EM in Pool-Based Active Learning for Text Classification, A. McCallum, K. Nigam, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/EmployingEMinPoolBasedActiveLearningForTextClassificationMcCallum98.pdf)

[Approximation Capabilities of Multilayer Feedforward Networks, Kurt Hornik, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ApproximationCapabilitiesOfMultilayerFeedforwardNetworksHornik1991.pdf)

[Approximation by Superpositions of a Sigmoidal Function, G. Cybenko, 1989](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ApproximationBySuperpositionsOfSigmoidalFunctionCybenko1989.pdf)

[Fast R-CNN, Ross Girshick, Microsoft Research, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Fast_R-CNN.pdf)

[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, S. Ren, K. He, Ross Girshick, Jian Sun, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Faster_R-CNN_Towards_Real-Time_Object_Detection_with_Region_Proposal_Networks.pdf)

[Feature Extraction using Convolution Neural Networks (CNN) and Deep Learning, M. Jogin, et al](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Feature_Extraction_Using_CNN_and_DeepLearning.pdf)

[Feature Representation In Convolutional Neural Networks, Ben Athiwaratkun et al, Cornell U, 2011](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Feature_Representation_in_CNN.pdf)

[Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Tech Report, UC Berkeley, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/RCNN_RichFeatureHierarchiesForAccurateObjectDetection.pdf)

[Pyramid Methods in Image Processing, E.H. Adelson, et al,  RCA, 1984](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/PyramidMethodsForImageProcessingRCA84.pdf)

[Learning Sparse Neural Networks Through L_0 Regularization, Christos Louizos, Max Welling, Diederik P. Kingma, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Sparse_Neural_Networks_Through_L0_Regularization_ICLR_2018.pdf)

[Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, Gal et al, Cambridge, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Dropout_as_a_Bayesian_Approximation-Representing_Model_Uncertainty_in_Deep_Learning_Gal_2016.pdf)

[Bayesian Learning for Neural Networks: Algorithmic Survey, Martin Margis, Alexandros Iosifidis, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bayesian_Learning_for_Neural_Networks-an_algorithmic_survey_2023.pdf)

[Bayesian Methods for Neural Networks, Joao de Freitas, Cambridge U., 2000, PhD thesis](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Bayesian_Methods_For_Neural_Networks_Freitas_PhD_thesis_CambdridgeU_2000.pdf)

[A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference, Kumar Shridhar et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Comprehensive_guide_to_Bayesian_Convolutional_Neural_Network_with_Variational_Inference_Shridhar_2019.pdf)

[Convex Bounds on the Softmax Function with Applications to Robustness Verification, Wei, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Convex_Bounds_on_the_Softmax_Function_with_Applications_to_Robustness_Verification_wei_23.pdf)

[Everything is Connected: Graph Neural Networks, Peter Velickovic, DeepMind, U Cambridge, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Everything_is_Connected-Graph_Neural_Networks_Petar_Velickovic_2023.pdf)

[A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges, Abdar, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Review_of_Uncertainty_Quantification_in_Deep_Learning-Techniques_Applications_and_Challenges_Abdar_2021.pdf)

[Solving olympiad geometry without human demonstrations, Trieu H. Trinh et al, DeepMind, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Solving_olympiad_geometry_without_human_demonstrations_Trinh_DeepMind_2023.pdf)

[HyperNetworks, David Ha, GoogleBrain, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Hypernetworks_David_Ha_GoogleBrain_2017.pdf)

[MotherNet: A Foundational Hypernetwork for Tabular Classification, A.C. Mueller et al, Microsoft Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MotherNet-A_Foundational_Hypernetwork_for_Tabular_Classification_Mueller_2023.pdf)

[Using Sequences of Life-events to Predict Human Lives, Germans Savcisens et al, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Using_Sequences_of_Life-events_to_Predict_Human_Lives_Savcisens_2023.pdf)

[Lie Group Decompositions for Equivariant Neural Networks, Mircea Mironenco, Patrick Forre, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Lie_Group_Decompositions_for_Equivariant_Neural_Networks_Mironenco_2023.pdf)

[What Every Computer Scientist Should Know About Floating-Point Arithmetic, D. Goldberg, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/WhatEveryScientistShouldKnowAboutFloatingPointNumbersGoldberg1991.pdf)

[One Pixel Attack for Fooling Deep Neural Networks, J. Su et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/One_Pixel_Attack_for_Fooling_Deep_Neural_Networks_Su_2019.pdf)

related youtube presentation: [here](https://youtu.be/_y1tIdnB__Y?si=kZyy0ldXFUWTuqYx)

[Exploring Adversarial Examples: Patterns of One-Pixel Attack, David Kügler et al, TU Darmstadt, 2019](https://ar5iv.labs.arxiv.org/html/1806.09410)

## Deep Learning for image processing

Resources on this [link](https://github.com/dimitarpg13/deep_learning_for_image_processing/blob/main/Resources.md)

## Sequence to Sequence and RNNs

[Recurrent Neural Networks Regularization, Wojciech Zaremba et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Recurrent_Neural_Networks_Regularization_Zaremba_2015.pdf)

[Order Matters: Sequence to Squence for Sets, Oriol Vinyals et al, Google Brain, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Order_Matters_Sequence_to_Sequence_for_Sets_Vinyals_GoogleBrain_2016.pdf)

[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy's blog, 2015](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

## Feature Learning
[Mechanism of feature learning in deep fully connected networks
and kernel machines that recursively learn features, A. Radhakrishnan, MIT, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Mechanism_of_feature_learning_in_deep_fully_connected_networks_and_kernel_machines_that_recursively_learn_features_Radhakrishnan_2023.pdf)

[Supplementary material for Mechanism for feature learning in neural networks and backpropagation-free machine learning models, A. Radhakrishnan et al, 2024, Science](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Mechanism_for_feature_learning_in_neural_networks_and_backpropagation-free_machine_learning_models_science_supplement_2024.pdf)

[Foundations of Machine Learning: Over-parameterization and Feature Learning, A. Radhakrishnan, PhD Thesis, MIT, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/radhakrishnan-aradha-phd-eecs-2023-thesis.pdf)

## Double Descent

[Understanding the Double Descent Phenomenon in DeepLearning, Marc Lafon, Alexandre Thomas, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Understanding_the_Double_Descent_Phenomenon_in_Deep_Learning_Lafon_2021.pdf)

[Reconciling modern machine learning practice and the bias-variance trade-of, Mikhail Belkin et al, The OSU, 2019 ](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Reconciling_modern_machine_learning_practice_and_the_bias-variance_trade-off_Belkin_2019.pdf)

[Onthe Optimization of Deep Networks: Implicit Acceleration by Overparameterization, S. Arora et al, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/On_the_Optimization_of_Deep_Networks-Implicit_Acceleration_by_Overparameterization_Arora_2018.pdf)

[Relational inductive biases, deep learning, and graph networks, Peter W Battaglia et al, DeepMind, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Relational_inductive_biases_deep_learning_and_graph_networks_Bataglia_2018.pdf)

## AutoEncoders

[Autoencoders, Dor Bank, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoders.pdf)

[Autoencoders, Unsupervised Learning, and Deep Architectures, Pierre Baldi, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoders_Unsupervised_Learning_and_Deep_Architectures_Baldi_2012a.pdf)

[Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima, Pierre Baldi, Kurt Hornik, 1988](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Neural_Networks_and_Principal_Component_Analysis-Learning_from_Examples_Without_Local_Minima_Baldi_Hornik-89.pdf)

[Neural Networks: Unleashing the Power of Latent Space Compression, Julien Pascal, Medium, 2023](https://medium.com/@julien.pascal/neural-networks-unleashing-the-power-of-latent-space-compression-2c8630f6f6cc)

[The Sparse Autoencoder, Andrew Ng, Lecture Notes CS294A](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/sparseAutoencoder_AndrewNg_LectureNotes.pdf)

## Attention

[Bidirectional Recurrent Neural Networks, Mike Schuster, Kuldip Paliwal, IEEE Transactions on Signal Processing,1997](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bidirectional_Recurrent_Neural_Networks_Schuster_Paliwal_1997.pdf)

[Neural Networks for Pattern Recognition, Christopher M. Bishop, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Neural_Networks_for_Pattern_Recognition-Bishop_1995.pdf)

[Recurrent Continuous Translation Models, Nal Kalchbrenner, Phil Blunsom, Oxford U.,2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Recurrent_continuous_translation_models_Kalchbrenner_Blunsson_OxfordU_2013.pdf)

[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, K. Cho, B. van Merrienboer, C. Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_Cho_UdMontreal_2014.pdf)

[Generating Sequences With Recurrent Neural Networks, Alex Graves, U of Toronto, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Generating_Sequences_With_Recurrent_Neural_Networks_Graves_2014.pdf)

[End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results, Jan Chorowski et al, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/End-to-end_Continuous_Speech_Recognition_using_Attention-based_Recurrent_NN-First_Results_Chorowski_2014.pdf)

[Translation Modeling with Bidirectional Recurrent Neural Networks, M. Sundermeyer et al, Aachen U., 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Translation_Modeling_with_Bidirectional_Recurrent_Neural_Networks_Sundermeyer_2014.pdf)

[LSTM Can Solve Hard Long Time Lag Problems, Sepp Hochreiter, Jurgen Schmidhuber NIPS, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1996-lstm-can-solve-hard-long-time-lag-problems-Paper_Hochreiter.pdf)

[Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation, Jean Pouget-Abadie et al, EPF, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Overcoming_the_Curse_of_Sentence_Length_for_Neural_Machine_Translation_using_Automatic_Segmentation_Puget-Abadie_EPF_2014.pdf)

[Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, E. Kiperwasser, Y. Goldberg,  2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simple_and_Accurate_Dependency_Parsing_Using_Bidirectional_LSTM_Feature_Representations_Kiperwasser_2016.pdf)

[Contextual Position Encoding: Learning to Count What's Important, O. Golovneva et al, Meta, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Contextual_Position_Encoding-Learning_to_Count_Whats_Important_Golovneva_Meta_2024.pdf)

## Embeddings

[Embeddings in Natural Language Processing: Theory and Advances in Vector Representation of Meaning, M. Pilhevar, J. Camacho-Collados, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Embeddings_in_NLP_Pilhevar_book_draft.pdf)

[Vector Semantics and Embeddings, Daniel Jurafsky and James H Martin, Speech and Language Processing, Chapter 6, Stanford U,, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Vector_Semantics_and_Embeddings_Jurafsky_2024.pdf)

[Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov et al, Google, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Efficient_Estimation_of_Word_Representations_in_Vector_Space_Mikolov_Google_2013.pdf)

[Factors Influencing the Surprising Instability of Word Embeddings, L. Wendtlandt et al, U. Michigan Ann Arbor, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Factors_Influencing_the_Surprising_Instability_of_Word_Embeddings_Wendlandt_AnnArbor_2018.pdf)

[Is Cosine-Similarity of Embeddings Really About Similarity? Harald Steck, Chaitanya Ekanadham, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Is_Cosine-Similarity_of_Embeddings_Really_About_Similarity_Steck_2024.pdf)

[Word Embeddings What Works, What Doesn't And How To Tell The Difference For Applied Research, A. Spirling, P. Rodriguez, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Word_Embeddings_Spirling_Rogriguez_2019.pdf)

[Word Embeddings - A Survey, Felipe Almeida, Geraldo Xexeo, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Word_Embeddings-A_Survey_Almeida_2023.pdf)

## Self-organizing maps

[The Self-Organizing Map, Kohonen, IEEE, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheSelfOrganizing_map_Kohonen_IEEE_1998.pdf)

[Kohonen Self-Organizing Map for the Traveling Salesperson Problem, Lucas Brocki, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Kohonen-Self-Organizing_Map_for_the_Traveling_Salesperson_Problem_Brocki_2010.pdf)

[Using Self-Organizing Maps to solve the Traveling Salesman Problem, Diego Vincente's blog](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Using_Self-Organizing_Maps_to_solve_the_Traveling_Salesman_Problem_Diego_Vicente_Blog.pdf)

[Diego Vincente's github repo on self-organizing maps for solving TSP](https://github.com/diego-vicente/som-tsp)

## State Space Models

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu, T. Dao, CMU, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2023.pdf)

[Mamba: Can it replace Transformers? Vishal Rajput, Medium Jan 8, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba_Can_it_replace_Transformers_Vishal_Rajput_Jan_2024_Medium.pdf)

[Why Mamba was rejected? Joe El Khoury, Medium, Feb 28, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Why_Mamba_was_rejected_recently_at_the_ICLR_by_Joe_Elkhoury_Feb_2024_Medium.pdf)

[A Visual Guide to Mamba and State Space Models, Maarten Grootendorst, 2024](https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6)

[Mamba Explained, Kola Ayonrinde, The Gradient, March 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba_Explained_The_Gradient.pdf)

[HiPPO: Recurrent Memory with Optimal Polynomial Projections, A. Gu et al, Stanford U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/HiPPO-Recurrent_Memory_with_Optimal_Polynomial_Projections_Gu_2020.pdf)

[How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections, A. Gu et al, Stanford U., 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/How_to_Train_Your_HiPPO-State_Space_Models_with_Generalized_Orthogonal_Basis_Projections_Gu_2022.pdf)

[Hungry Hungry Hippos: Towards Language Modeling with State Space Models, D. Fu, T. Dao, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Hungry_Hungry_Hippos-Towards_Language_Modeling_with_State_Space_Models_Fu_Dao_Stanford_2023.pdf)

[Efficiently Modeling Long Sequences with Structured State Spaces, K. Goel, A. Gu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Efficiently_Modeling_Long_Sequences_with_Structured_State_Spaces_Gu_Stanford_2022.pdf)

[Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers, A. Gu et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Combining_Recurrent_Convolutional_and_Continuous-time_Models_with_Linear_State-Space_Layers_Gu_2021.pdf)

[Diagonal State Spaces are as Effective as Structured State Spaces, A. Gupta, A. Gu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Diagonal_State_Spaces_are_as_Effective_as_Structured_State_Spaces_Gupta_Gu_2022.pdf)

## Graph Neural Networks

[Graph Representation Learning, W. Hamilton, McGill U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Graph_Representation_Learning_Book_Hamilton_2020.pdf)

[DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs, D. Zheng et al, AWS AI, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DistDGL-Distributed_Graph_Neural_Network_Training_for_Billion-Scale_Graphs_Zheng_AWS_AI_2023.pdf)

[Temporal Graph Learning in 2024: continue the journey for evolving networks, S. Huang, Towards Data Science, 2024](https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2)

[TGL: A General Framework for Temporal GNN Training on Billion-Scale Graphs, H. Zhou, U o SoCal, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/tgl-a-general-framework-for-temporal-gnn-training-on-billion-scale-graphs-scalable-data-science.pdf)

[Graph neural networks in TensorFlow, Dustin Zelle, Distin Zelle, Feb 6, 2024, online blog](https://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html)

## Long Short Term Memory (LSTM)

[xLSTM: Extended Long Short-Term Memory, Maximilian Beck et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/xLSTM-Extended_Long_Short-Term_Memory_Beck_2024.pdf)

[Long Short Term Memory, Sepp Hochreiter, 1997](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/LongShortTermMemory.pdf)

[LSTM Can Solve Hard Long Time Lag Problems, Sepp Hochreiter, Jurgen Schmidhuber NIPS, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1996-lstm-can-solve-hard-long-time-lag-problems-Paper_Hochreiter.pdf)

[Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, E. Kiperwasser, Y. Goldberg,  2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simple_and_Accurate_Dependency_Parsing_Using_Bidirectional_LSTM_Feature_Representations_Kiperwasser_2016.pdf)

[Understanding LSTM: a Tutorial into Long Short Term Memory Recurrent Neural Networks, Ralf C. Staudemeyer et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TutorialOnLongShortTermMemory2019.pdf)

[Understanding LSTM networks, Colah's blog, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/UnderstandingLSTMNetworks-colahsblog.pdf)

[Fundamentals of RNN and LSTM Network, Alex Sherstinsky, MIT, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/FundamentalsOfRNNandLSTMNetwork.pdf)

## Gated Recurrent Units (GRU)

[A Dynamic Regime-Switching Model Using Gated Recurrent Straight-Through Units, N. Antulov-Fantulin, A. Cauderan, Petter N. Kolm, NYU Courant Institute, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/gated_recurrent_unit/A_Dynamic_Regime-Switching_Model_Using_Gated_Recurrent_Straight-Through_Units_Fantoulin_Cauderas_Kolm_2024.pdf)

## Theorem Proving
[NeurIPS Tutorial on Machine Learning for Theorem Proving, video](https://machine-learning-for-theorem-proving.github.io/)

[DeepMath - Deep Sequence Models for Premise Selection, Alexander Alemi, Francois Chollet et al, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/theorem_proving/DeepMath-Deep_Sequence_Models_for_Premise_Selection_alemi_2016.pdf)

## Computability of Neural Networks

[Turing Computability with Neural Nets, Hava T. Siegelmann, Eduardo Sontag, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/TuringComputabilityWithNeuralNets_Siegelman1991.pdf)

[On the computational power of Neural Nets, Hava T. Siegelmann, 1992 (earlier version)](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/OnTheComputationalPowerOfNeuralNets_1992_Siegelmann.pdf)

[On The Computation Power of Neural Nets, Hava T. Siegelmann, Eduardo Sontag, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/OnTheComputationalPowerOfNeuralNets_1995_Siegelmann_JComSysSci.pdf)

[Computation beyond Turing limit, Hava T. Siegelmann, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/ComputationBeyondtheTuringLimit_1995_Siegelmann_Science.pdf)

[Neural Networks and Analog Computation beyond Turing limit, 1999](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/NeuralNetworksandAnalogComputationBeyondTheTuringLimit.pdf)

[Efficient Simulation of Finite Automata by Neural Nets, Noga Alon et al, JACM, Vol. 38, No. 2, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/Efficient_simulations_of_finite_automata_by_neural_nets_Alon_1991.pdf)

## Vector Databases

[Foundations of Vector Retrieval, Sebastian Bruch, 2024, arXiv](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Foundations_of_Vector_Retrieval_Bruch_2024.pdf)

[A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge, Y. Han et al, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Comprehensive_Survey_on_Vector_Database-Storage_and_Retrieval_Technique_Challenge_Han_2023.pdf)

[Vector database management systems: Fundamental concepts, use-cases, and current challenges, T. Taipalus, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Vector_database_management_systems-Fundamental_concepts_use-cases_and_current_challenges_Taipalus_2023.pdf)

[Embeddings, Vector Databases and Search, Module 2 slides from Databricks course, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Embedding_Vector_DBs_and_Search_Databricks_slides_2023.pdf)

## Deep Learning for Physical Applications

[Predicting disruptive instabilities in controlled fusion plasmas through deep learning, Julian Kates-Harbeck et al, Harvard U, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Predicting_disruptive_instabilities_in_controlled_fusion_plasmas_through_deep_learning_Kates-Harbeck_Harvard_2019.pdf)

[Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning, Jonas Degrave, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Magnetic_control_of_tokamak_plasmas_through_deep_reinforcement_learning_Degrave_DeepMind_2019.pdf)

[First Nuclear Plasma Control with Digital Twin, Sabine Hossenfelder, Feb 2024, youtube video](https://youtu.be/4VD_DLPQJBU)

[Avoiding fusion plasma tearing instability with deep reinforcement learning, Jaemin Seo et al, Nature, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Avoiding_fusion_plasma_tearing_instability_with_deep_reinforcement_learning_Seo_2024.pdf)

## Time Series Forecasting

[Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series, V. Ekambaram et al, IBM, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Tiny_Time_Mixers-Fast_Pre-trained_Models_or_Enhanced_Zero_Few-Shot_Forecasting_of_Multivariate_Time_Series_Ekambaram_2024.pdf)

[A decoder-only foundation model for time-series forecasting, A. Das et al, Google, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/A_decoder-only_foundation_model_for_time-series_forecasting_Das_Google_April_2024_Preprint.pdf)

[MLP-Mixer: An all-MLP Architecture for Vision, Ilya Tolstikhin et al, Google, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/MLP-Mixer-An_all-MLP_Architecture_for_Vision_Tolstikhin_Google_2021.pdf)

[TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting, V. Ekambaram et al, IBM, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/TSMixer-Lightweight_MLP-Mixer_Model_for_Multivariate_Time_Series_Forecasting_Ekambaram_IBM_2023.pdf)

[TSMixer: An All-MLP Architecture for Time Series Forecasting, SA Chen et al, Google, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/TSMixer-An_All-MLP_Architecture_for_Time_Series_Forecasting_Chen_Google_2023.pdf)

[Predicting Venetian Lagoon Tide Levels with Multivariate Time Series Modeling, David Proietti, 2024](https://medium.com/@david.proietti_17/predicting-venetian-lagoon-tide-levels-with-multivariate-time-series-modeling-8bafdf229588)

[iTransformer: The Latest Breakthrough in Time Series Forecasting, Marco Peixeiro, Towards Data Science, April 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/iTransformer-Inverted_Transformers_Are_Effective_for_Time_Series_Forecasting_Liu_2023.pdf)

[Unified Training of Universal Time Series Forecasting Transformers, Woo, G et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Unified_Training_of_Universal_Time_Series_Forecasting_Transformers_Woo_2024.pdf)

[Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)

[How to Effectively Forecast Time Series with Amazon's New Time Series Forecasting Model, Eivind Kjosbakken, April 9, 2024, Towards Data Science](https://towardsdatascience.com/how-to-effectively-forecast-time-series-with-amazons-new-time-series-forecasting-model-9e04d4ccf67e)

[N-BEATS — The First Interpretable Deep Learning Model That Worked for Time Series Forecasting, Jonte Dancker, 2024](https://towardsdatascience.com/n-beats-the-first-interpretable-deep-learning-model-that-worked-for-time-series-forecasting-06920daadac2)

[N-BEATS: Neural basis expansion analysis for interpretable time series forecasting, BN Oreshkin et al, Element AI, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/N-BEATS-Neural_basis_expansion_analysis_for_interpretable_time_series_forecasting_Oreshkin_ElementAI_2020.pdf)


## medium
[The Math Behind the Adam Optimizer with Cristian Leo, 2024](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b)

[Ensemble Learning: Bagging and Boosting with Jonas Dieckmann](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0)

[Pruning Neural Networks, Rohit Bandaru, 2020](https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9)

[Nesterov Momentum Explained with examples in TensorFlow and PyTorch with Giorgio Martinez, 2022](https://medium.com/@giorgio.martinez1926/nesterov-momentum-explained-with-examples-in-tensorflow-and-pytorch-4673dbf21998)

[An Overview for Model Compression Techniques for Deep Learning in Space with Hannah Peterson](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5)

[Gentle Introduction to Bayesian Deep Learning with François Porcher](https://pub.aimind.so/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)

[Why Deep Learning Ensembles Outperform Bayesian Neural Networks with Devansh](https://medium.com/swlh/why-deep-learning-ensembles-outperform-bayesian-neural-networks-dba2cd34da24)

[Bayesian Neural Network Series Post 1: Need for Bayesian Neural Networks with Kumar Shridhar](https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2)

[Bayesian Neural Network Series Post 2: Background Knowledge with Kumar Shridhar](https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43)

[Spectral Graph Convolutions with Jose Luis Castro Garica](https://medium.com/@jlcastrog99/spectral-graph-convolutions-c7241af4d8e2)

[TS2Vec review: Towards universal time series representation learning with hierarchical contrasting with Alexander Li](https://medium.com/@findalexli/ts2vec-review-towards-universial-time-series-representation-learning-with-hierarchical-contra-c50dd789e85c)

[Is PyTorch's Nesterov Momentum Implementation Worng? with Jason Vega](https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008)

[Effective Load Balancing with Ray on Amazon SageMaker with Chaim Rand](https://towardsdatascience.com/effective-load-balancing-with-ray-on-amazon-sagemaker-d3b9020679d3)

[Layerwise learning for Quantum Neural Networks with Qiskit with Gopal Dahale](https://medium.com/qiskit/layerwise-learning-for-quantum-neural-networks-with-qiskit-e17ff4b1c419)

[Forward-Forward Algorithm: Will it replace Backpropagation? with Vishal Rajput](https://medium.com/aiguys/forward-forward-algorithm-will-it-replace-backpropagation-8d9047192975)

[Tabula Rasa: Why Do Tree-Based Algorithms Outperform Neural Networks with Salvatore Raieli](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b)

[DeepMind Decodes the Puzzle of ‘ Grokking ’ In Neural Network Generalization Through Circuit Efficiency](https://medium.com/syncedreview/deepmind-decodes-the-puzzle-of-grokking-in-neural-network-generalization-through-circuit-b5bc32380557)

   related paper: [Explaining Grokking Through Circuit Efficiency, Varma et al, DeepMind, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Explaining_grokking_through_circuit_efficiency_Varma_DeepMind_2023.pdf)

[Liquid Neural Network : A adaptive way to train ML model with Ved Prakash](https://ved933409.medium.com/liquid-neural-network-a-adaptive-way-to-train-ml-model-b6922d755066)


[Understanding Zero-Shot Learning — Making ML More Human with Ekin Tiu](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab)

 related paper: [Learning Transferable Visual Models From Natural Language Supervision, Alec Radford et al, OpenAI, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision_Radford_2021.pdf)

 related paper: [Simulation to Scaled City: Zero-Shot Policy Transfer for Traffic Control via Autonomous Vehicles, Kathy Jang, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simulation_to_Scaled_City-Zero-Shot_Policy_Transfer_for_Traffic_Control_via_Autonomous_Vehicles_Jang_UCBerkeley_2019.pdf)

 related paper: [Zero-Shot Learning and its Applications from Autonomous Vehicles to COVID-19 Diagnosis: A Review, Rezaei et al, U Leeds, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Zero-Shot_Learning_and_its_Applications_from_Autonomous_Vehicles_to_COVID-19_Diagnosis-A_Review_Rezael_ULeeds_2020.pdf)

[Conditional Variational Autoencoders with Learnable Conditional Embeddings with Tim Rose](https://towardsdatascience.com/conditional-variational-autoencoders-with-learnable-conditional-embeddings-e22ee5359a2a)

[Differential Equations as a Pytorch Neural Network Layer with Kevin Hannay](https://towardsdatascience.com/differential-equations-as-a-pytorch-neural-network-layer-7614ba6d587f)

[ULTRA: Foundation Models for Knowledge Graph Reasoning with Michail Galkin](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09)

related paper: [A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs, X. Huang et al, Oxford U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Theory_of_Link_Prediction_via_Relational_Weisfeiler-Leman_on_Knowledge_Graphs_Huang_Oxford_2023.pdf)

related paper: [Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types, J. Gao et al, Purdue U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Double_Equivariance_for_Inductive_Link_Prediction_for_Both_New_Nodes_and_New_Relation_Types_Gao_PurdueU_2023.pdf)

related paper: [Edge Directionality Improves Learning on Heterophilic Graphs, Rossi et al, Imperial College, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Edge_Directionality_Improves_Learning_on_Heterophilic_Graphs_Rossi_2023.pdf)

related paper: [How Powerful are Graph Neural Networks? K. Xu et al, MIT, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/How_Powerful_are_Graph_Neural_Networks_Xu_MIT_2019.pdf)

related paper: [On The Equivalence Between Positional Node Embeddings and Structural Graph Representation, Srinivasan et al, Purdue U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/On_the_Equivalence_between_Positional_Node_Embeddings_and_Structural_Graph_Representations_Srinivasa_ISLR_2020.pdf)

related paper: [The Graph Neural Network Model, F. Scarcelli et al, U of Sienna, 2009](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheGraphNeuralNetworkModelScarselli2009.pdf)

related paper: [Towards Foundation Models for Knowledge Graph Reasoning, M. Galkin et al, Intel AI Lab, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Towards_Foundation_Models_for_Knowledge_Graph_Reasoning_Galkin_2023.pdf)

related paper: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks, C Morris et al, TU Dortmund, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Weisfeiler_and_Leman_Go_Neural-Higher-order_Graph_Neural_Networks_Morris_2018.pdf)

related paper: [Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction, Zhu et al, Quebec AI Inst., 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Neural_Bellman-Ford_Networks-A_General_Graph_Neural_Network_Framework_for_Link_Prediction_Quebec_AI_Inst_Zhu_2022.pdf)

related paper: [NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs, M. Galkin et al, McGill, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NodePiece-Compositional_and_Parameter-Efficient_Representations_of_Large_Knowledge_Graphs_Galkin_McGill_2022.pdf)

[A gentle introduction to Steerable Neural Networks (part 1) with Matteo Ciprian](https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f)

related paper: [3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data, M. Weller et al, U Amsterdam 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/3D_Steerable_CNNs-Learning_Rotationally_Equivariant_Features_in_Volumetric_Data_Weller_UAmsterdam_2018.pdf)

related paper: [Steerable CNNs, Taco S Cohen, Max Welling, U. Amsterdam, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Steerable_CNNs_Cohen_UAmsterdam_2016.pdf)

related paper: [Learning Steerable Filters for Rotation Equivariant CNNs, Maurice Weiler et al, U Amsterdam, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Steerable_Filters_for_Rotation_Equivariant_CNNs_Weiler_UAmsterdam_2018.pdf)

related paper: [General E(2)-Equivariant Steerable CNNs, Maurice Weiler, Gabriele Cesa, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/General_E2-Equivariant_Steerable_CNNs_Weiler_2018.pdf)

related paper: [Scale Steerable Filters for the Locally Scale-Invariant Convolutional Neural Network, R. Ghosh et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Scale_Steerable_Filters_for_the_Locally_Scale-Invariant_Convolutional_Neural_Network_Ghosh_2019.pdf)

related paper: [A program to build E(n)-equivariant steerable CNNs, Cesa et al, U Amsterdam, Qualcomm AI Research, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_program_to_build_E_n-equivariant_steerable_CNNs_Cesa_2022.pdf)

[TiDE: the ‘embarrassingly’ simple MLP that beats Transformers with Rafael Guedes](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079)

related paper: [Long-term Forecasting with TiDE: Time-series Dense Encoder, Das et al, Google Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Long-term_Forecasting_with_TiDE-Time-series_Dense_Encoder_Das_GoogleResearch_2023.pdf)

related paper: [Darts: User-Friendly Modern Machine Learning for Time Series, Herzen et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Darts-User-Friendly_Modern_Machine_Learning_for_Time_Series_Herzen_2022.pdf)

related paper: [GluonTS: Probabilistic Time Series Models in Python, A. Alexandrov et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/GluonTS-Probabilistic_Time_Series_Models_in_Python_Alexandrov_AWS_2019.pdf)

related paper: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting, B Lim et al, Oxford U., 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Temporal_Fusion_Transformers_for_Interpretable_Multi-horizon_Time_Series_Forecasting_Lim_OxfordU_2020.pdf)

related paper: [An All-MLP Architecture for Time Series Forecasting, S. Chen et al, Google Cloud AI Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TSMixer-An_All-MLP_Architecture_for_Time_Series_Forecasting_Chen_GoogleCloudAIResearch_2023.pdf)

### Physics based Deep Learning

[Unraveling the Design Pattern of Physics-Informed Neural Networks: Series 01 with Shuai Guo](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)

[Operator Learning via Physics-Informed DeepONet: Let’s Implement It From Scratch with Shuai Guo](https://towardsdatascience.com/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)

   related paper: [DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators, L. Lu et al, Brown U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DeepONet-Learning_nonlinear_operators_for_identifying_differential_equations_based_on_the_universal_approximation_theorem_of_operators_Lu_BrownU_2020.pdf)

   related paper: [Learning the Solution Operator of Paramteric Partial Differential Equations with Physics-Iinformed DeepONets, S. Wang, U Penn, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_the_Solution_Operator_of_Parametric_Partial_Differential_Equations_with_Physics-Informed_DeepONets_Wang_UPenn_2021.pdf)

[Discovering Differential Equations with Physics-Informed Neural Networks and Symbolic Regression with Shuai Guo](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)

   related paper: [Interpretable Machine Learning for Science
with PySR and SymbolicRegression.jl, M. Carnmer, Princeton U., 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Interpretable_Machine_Learning_for_Science_with_PySR_and_SymbolicRegression.jl_Carnmer_2023.pdf)

   related paper: [Discovering a reaction-diffusion model for Alzheimer’s disease by
combining PINNs with symbolic regression, Z. Zhang, et al, Brown U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Discovering_a_reaction-diffusion_model_for_Alzheimers_disease_by_combining_PINNs_with_symbolic_regression_Zhang_StanfordU_2023.pdf)

   related repo: https://github.com/ShuaiGuo16/PINN_symbolic_regression

[Solving Inverse Problems With Physics-Informed DeepONet: A Practical Guide With Code Implementation with Shuai Guo](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)

### ML Ops for deep learning 
[Instance Selection for Deep Learning with Chaim Rand](https://towardsdatascience.com/instance-selection-for-deep-learning-7463d774cff0)

[Effective Load Balancing with Ray on Amazon SageMaker with Chaim Rand](https://towardsdatascience.com/effective-load-balancing-with-ray-on-amazon-sagemaker-d3b9020679d3)

[PyTorch Model Performance Analysis and Optimization (Part 1) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869)

[PyTorch Model Performance Analysis and Optimization (Part 2) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-2-3bc241be91)

[PyTorch Model Performance Analysis and Optimization (Part 3) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-3-1c5876d78fe2)

[PyTorch Model Performance Analysis and Optimization (Part 4) with Chaim Rand](https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9)

[PyTorch Model Performance Analysis and Optimization (Part 5) with Chaim Rand](https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206)

[PyTorch Model Performance Analysis and Optimization (Part 6) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-6-b87412a0371b)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 1) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 2) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-2-c833e7139b5f)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 3) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-3-db707db8a202)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 1) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-1-7877cdd6e879)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 2) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-2-23b1dac5ebed)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 3) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-3-f66dddbe1415)

[Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation), Lucas de Lima Nogueira, 2024, Towards Data Science](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc)

## Yan LeCun's Deep Learning Course at Center for Data Science, NYU

https://cds.nyu.edu/deep-learning/

[Transforming Deep Learning Education with Yann LeCun and Alfredo Canziani’s Free Online Course, NYU Center for Data Science, Medium](https://nyudatascience.medium.com/transforming-deep-learning-education-with-yann-lecun-and-alfredo-canzianis-free-online-course-6cccfd1970b3)

## online videos

### Neural Networks - from zero to hero, Andrej Karpathy, Feb 2023

[The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)

[The spelled-out intro to language modeling: building makemore](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 2: MLP](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc)

[Building makemore Part 4: Becoming a Backprop Ninja](https://youtu.be/q8SA3rM6ckI)

[Building makemore Part 5: Building a WaveNet](https://youtu.be/t3YJ5hKiMQ0)

## Relevant Repos

* [Intro to Deep Learning (Tebs Labs fork)](https://github.com/dimitarpg13/intro-to-deep-learning/tree/master) 

