# Deep Learning and Neural Networks Resources

## Books

[Perceptrons: An Introduction to Computational Geometry, M. Minsky, S. Pappert, 1969, Expanded Edition, 3rd printing 1988](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Minsky-and-Papert-Perceptrons-1969.pdf)

[Foundations of Machine Learning, Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Foundations_of_Machine_Learning_Mohri_2012.pdf)

[The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks, Daniel A. Roberts, Sho Yaida, Boris Hanin, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Principles_of_Deep_Learning_Theory_Roberts_2021.pdf)

[Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory, Arnulf Jentzen, Benno Kuckuck, Phillipe von Wurstemberger, University of Muenster, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Mathematical_Introduction_to_Deep_Learning-Methods_Implementations_and_Theory_Jentzen_2023.pdf)

[Mathematical Theory of Deep Learning, Philipp Petersen, Jakob Zech, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Mathematical_theory_of_deep_learning_Petersen_2024.pdf)

[The Theory of Deep Learning, Sanjeev Arora, Princeton U., 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Theory_of_Deep_Learning_Sanjeev_Arora.pdf)

[Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, Bernhard Schoelkopf, Alexander J. Smola, MIT, 2002](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/scholkopf2002learning_with_kernels.pdf)

[Deep Learning: Foundations and Concepts, Christopher Bishop, Hugh Bishop, Cambridge, UK, 2023, online viewing only](https://issuu.com/cmb321/docs/deep_learning_ebook)

[Understanding Deep Learning, Simon J. Prince, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/UnderstandingDeepLearning_13_10_23_C.pdf)
(book site URL: https://udlbook.github.io/udlbook/)

[Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/deeplearning_latest_edition.pdf)

[Neural Networks - Systematic Introduction, Raul Rojas, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/neuron.pdf)

[Machine Learning - A Probabilistic Perspective, Kevin P. Murphy, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/MachineLearning-AProbabilisticPerspective.pdf)

[Pattern Recognition and Machine Learning, Christopher Bishop, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Bishop-Pattern_Recognition_and_Machine_Learning.pdf)

[Graph Neural Networks: Foundations, Frontiers, and Applications, Lingfei Wu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/GraphNeuralNetworks-FoundationsFrontiers_and_Applications_Wu_2022.pdf)

[Introduction to Graph Neural Networks, Zhiyuan Liu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Intro_to_Graph_Neural_Networks_Liu_2022.pdf)

[Deep Learning on Graphs, Yao Ma et al, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/DeepLearning_on_Graphs_Ma_2020.pdf)

[Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges, M. Bronstein et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Geometric_Deep_Learning_Grids_Groups_Graphs_Geodesics_and_Gauges_Bronstein_2021.pdf)

[Mathematical Foundations of Geometric Deep Learning, Haitz Saez de Ocariz Borde, Michael Bronstein, U. of Oxford, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Mathematical_Foundation_of_Geometrical_Deep_Learning_Borde_UoO_2025.pdf)

[Machine Learning Refined - Foundations, Algorithms, and Applications, Jeremy Watt, Reza Bohrani, Aggelos Katsaggelos, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Machine_Learning_Refined.pdf)

[Foundations of Vector Retrieval, Sebastian Bruch, 2024, arXiv](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Foundations_of_Vector_Retrieval_Bruch_2024.pdf)

[Deep Learning and Computational Physics, Lecture Notes, USC, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Deep_Learning_and_Computational_Physics_LectureNotes_2004.pdf)

[The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning, RY Rubinstein, DP Kroese, 2004](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Cross_Entropy_Method_A_Unified_Approach_Rubinstein_Kroese_2004.pdf)

[Dive into Deep Learning, Interactive deep learning book with code, math, and discussions, Aston Zhang, Zachary Lipton, Mu Li, Alexander Smola, online version](https://d2l.ai/index.html)

[Mathematics for Inference and Machine Learning, Marc Deisenroth, Stefanos Zafeiriou, Lecture Notes, Imperial College, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/InferenceAndMachineLearningNotes_Deisenroth_ImperialCollege_2017.pdf)

[Statistical Mechanics of Neural Networks, H. Huang, Springer, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Huang-StatMechNeuralNet-Springer21.pdf)

[The Fundamentals of Heavy Tails: Properties, Emergence, and Estimation, Jayakrishnan Nair, Adam Wierman, Bert Zwart, 2021, Draft](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Fundamentals_of_Heavy_Tails_Properties_Emergence_Estimation_Nair_2021_Draft.pdf)

[Probabilistic Artificial Intelligence, Andreas Krause, Jonas Hübotter, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Probabilistic_Artificial_Intelligence_Krause_Huebotter_2025.pdf)

[Patterns, Predictions, and Actions, Moritz Hardt and Benjamin Recht, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Patterns_Predictions_and_Actions_Hardt_2024.pdf)

[A Brief Introduction to Neural Networks, David Kriesel, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Brief_Intro_to_Neural_Networks_Kriesel_2025.pdf)

[Lectures on Neural Dynamics, Francesco Bullo, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Lectures_Neural_Dynamics_Bullo_2025.pdf)

[The Elements of Differentiable Programming, M. Blondel, Vincent Roulet, Google DeepMind, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/The_Elements_of_Differentiable_Programming_Blondel_2025.pdf)

## Lecture Notes

[Optimization for Machine Learning Lecture Notes CS-439, Spring 2025 Bernd Gartner, ETH, Martin Jaggi, EPFL](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/lecture_notes/Optimization_for_Machine_Learning_Gaertner_Lecture_Notes_CS-439_Spring_2025.pdf)

## Articles and tutorials
[A Logical Calculus of the Ideas Immanent in Nervous Activity, WS McCulloch, Walter Pitts, 1943](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mccolloch.logical.calculus.ideas.1943.pdf)

[On The Convergence Proofs for Perceptrons, Albert BJ Novikoff, Office of Naval Research, 1963](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/On_Convergence_Proofs_for_Perceptrons_Novikoff_1963.pdf)

[Learning Representations by Back-propagating Errors, D. Rumelhart, G. Hinton, R. Williams,Nature, 1986](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_representations_by_backpropagating_errors_Rummelhart_Hinton_Williams_1986.pdf)

[Perceptrons: An Introduction to Computational Geometry, M. Minsky, S. Pappert, 1969, Expanded Edition, 3rd printing 1988](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Minsky-and-Papert-Perceptrons-1969.pdf)

[Multilayer Feedforward Networks are Universal Approximators, Kurt Hornik, Maxwell Stinchcombe, Halber White, 1989](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MultilayerFeedforwardNetworksAreUniversalApproximatorsHornik89.pdf)

[Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function, M. Leshno, V. Y. Lin, A. Pinkus, S. Schocken, 1993](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MultilayerFeedforwardNetworksWithNonpolynomialActivationFunctionCanApproximateAnyFunctionLeshno1993.pdf)

[Large Margin Classification Using the Perceptron Algorithm, Yoav Freund, Robert S. Schapire, 1999](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Large_Margin_Classification_Using_the_Perceptron_Algorithm_Freund_Schapire_1999.pdf)

[ImageNet Classification with Deep Convolutional Neural Networks, Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, NIPS, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf)

[Reducing the Dimensionality of Data with Neural Networks, G. Hinton, R. Salakhutdinov, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ReducingTheDimensionalityOfDataWithNeuralNetsHinton2006.pdf)

[Transforming Auto-encoders, G. Hinton, A. Krizhevsky, S.D. Wang, 2011](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TransformingAutoencodersHinton.pdf)

[On the Importance of Initialization and Momentum in Deep Learning, Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/OnTheImportanceOfInitializationAndMomentumInDeepLearningSutskever13.pdf)

[Distilling the Knowledge in a Neural Network, G. Hinton et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Distilling_the_Knowledge_in_a_Neural_Network_Hinton_2015.pdf)

[Multiple Object Recognition with Visual Attention, Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Multiple_Object_Recognition_with_Visual_Attention_Ba_Mnih_2015.pdf)

[The Forward-Forward Algorithm: Some Preliminary Investigations, Geoffrey Hinton, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/The_Forward-Forward_Algorithm-Some_Preliminary_Investigations_Hinton_2022.pdf)

[Neural Networks and Deep Learning, Michael Nielsen, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NeuralNetworksAndDeepLearningNielsen.pdf)

[Autoencoding Video Frames, Terence Broad, Master Thesis, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoding_Video_Frames_Broad_2016.pdf)

[Deep Learning: Methods and Applications, Li Deng, Dong Yu, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)

[Universal Approximation of an Unknown Mapping and Its Derivatives Using Multilayer Feedforward Networks, Kurt Hornik, Maxwell Stinchcombe, Halbert White, 1990](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/UniversalApproximationOfUnknownMappingHrnik90.pdf)

[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, J. Frankle, M. Carbin MIT CSAIL 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheLotteryTicketHypothesisFindingSparseTrainableNeuralNetworksFrankle2017.pdf)

[A Theory of Transfer Learning with Applications of Active Learning, Liu Yang, Steve Hanneke, Jaime Carbonell, Carnegie Mellon University, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheoryOfTransferLearningWithApplicationsToActiveLearningCMUYang.pdf)

[Learning with Local and Global Consistency, D. Zhou, O. Bousquet, T. Lal, J. Weston, B. Scholkopf, 2003](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-2003-learning-with-local-and-global-consistency-Paper.pdf)

[Bagging, Boosting and Ensemble Methods, Peter Buehlmann, ETH Zurich, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bagging_Boosting_and_Ensemble_Methods_Buhlmann_2010.pdf)

[Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition, Dominik Scherer, Andreas Mueller, Sven Behnke, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/EvalutationOfPoolingOperationsInConvolutionalArchitecturesForObjectRecognition.pdf)

[Learning Deep Architectures for AI, Yoshua Bengio, 2009](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Deep_Architectures_for_AI_Y_Bengio_2009.pdf)

[Learning both Weights and Connections for Efficient Neural Networks, Song Han et al, Stanford U, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/learning-both-weights-and-connections-for-efficient-neural-network-Han-Pool-Tran-Dally-2015.pdf)

[Boosting Algorithms: Regularization, Prediction and Model Fitting, Peter Buehlmann, Torsten Hothorn, 2008](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/BoostingAlgorithmsRegularizationPredictionAndModelFittingBuehlman2007.pdf)

[Least Angle Regression, Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, 2004](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/LeastAngleRegressionEffron2004.pdf)

[Regression Shrinkage and Selection via the Lasso, Robert Tibshirani, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/RegressionShrinkageAndSelectionViaTheLassoTibshirani1996.pdf)

[A Note on Learning Vector Quantization, Virginia de Sa, 1992](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1992-a-note-on-learning-vector-quantization-Paper.pdf)

[Learning Vector Quantization: The Dynamics of Winner-Takes-All Algorithms, M. Biehl et al, 2006](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Vector_Quantization-The_Dynamics_of_Winner-Takes-All_Algorithms_Biehl_2006.pdf)

[Six Lectures on Linearized Neural Networks, T. Misiakiewicz, A. Montanari, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/SixLecturesonLinearizedNeuralNetworks_Misiakiewicz_2023.pdf)

[The Modern Mathematics of Deep Learning, J. Berner et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ModernMathematicsOfDeepLearning2021.pdf)

[Introduction To Metric Fixed Point Theory, M.A. Khamsi, 2002](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/IntroductionToMetricFIxedPointTheoryKhamsi.pdf)

[A Global Geometric Framework for Nonlinear Dimensionality Reduction, J. Tenenbaum, et al, 2000](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/GlobalGeometricFrameworkForNonlinearDimensionalityReductionTenenbaum2000.pdf)

[Online Learning and Stochastic Approximations, Leon Bottou, AT&T Labs, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/OnlineAlgorithmsAndStochasticApproximationsBottou1998.pdf)

[A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, Yoav Freund, Robert E Schapire, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/decision-theoretic_generalization_of_online_learning_and_app_to_Boosting_Freund1996.pdf)

[Improving Generalization With Active Learning, David Cohn, Les Atlas, Richard Landner, 1994](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Cohn1994_ImprovingGeneralizationWithActiveLearning.pdf)

[High-Dimensional Dynamics of Generalization Error in Neural Networks, M.S. Advani et al, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/High-dimensional_dynamics_of_generalization_error_in_neural_networks_Advani_2017.pdf)

[Pruning Convolutional Neural Networks for Resource Efficient Inference, P. Molchanov, S. Tyree, T. Karras, T. Aila, J. Kautz, NVIDIA, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/PruningCNNForResourceEfficientInferenceMolchanov2017.pdf)

[What is The State of Neural Network Pruning? D. Blalock, J. Ortiz, J. Frankle, J. Guttag, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/WhatIsTheStateOfNeuralNetworkPruningBlalock2020.pdf)

[Deep Ensembles: A Loss Landascape Perspective, S. Fort et al, Google Research, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Deep_Ensembles-A_Loss_Landscape_Perspective_Fort_GoogleBrain_2020.pdf)

[You Only Look Once: Unified, Real-Time Object Detection, J. Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/YOLO-UnifiedRealTimeObjectDetection.pdf)

[Employing EM in Pool-Based Active Learning for Text Classification, A. McCallum, K. Nigam, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/EmployingEMinPoolBasedActiveLearningForTextClassificationMcCallum98.pdf)

[Approximation Capabilities of Multilayer Feedforward Networks, Kurt Hornik, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ApproximationCapabilitiesOfMultilayerFeedforwardNetworksHornik1991.pdf)

[Approximation by Superpositions of a Sigmoidal Function, G. Cybenko, 1989](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/ApproximationBySuperpositionsOfSigmoidalFunctionCybenko1989.pdf)

[Fast R-CNN, Ross Girshick, Microsoft Research, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Fast_R-CNN.pdf)

[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, S. Ren, K. He, Ross Girshick, Jian Sun, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Faster_R-CNN_Towards_Real-Time_Object_Detection_with_Region_Proposal_Networks.pdf)

[Feature Extraction using Convolution Neural Networks (CNN) and Deep Learning, M. Jogin, et al](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Feature_Extraction_Using_CNN_and_DeepLearning.pdf)

[Feature Representation In Convolutional Neural Networks, Ben Athiwaratkun et al, Cornell U, 2011](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Feature_Representation_in_CNN.pdf)

[Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation, Tech Report, UC Berkeley, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/RCNN_RichFeatureHierarchiesForAccurateObjectDetection.pdf)

[Pyramid Methods in Image Processing, E.H. Adelson, et al,  RCA, 1984](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/PyramidMethodsForImageProcessingRCA84.pdf)

[Learning Sparse Neural Networks Through L_0 Regularization, Christos Louizos, Max Welling, Diederik P. Kingma, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Sparse_Neural_Networks_Through_L0_Regularization_ICLR_2018.pdf)

[Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, Gal et al, Cambridge, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Dropout_as_a_Bayesian_Approximation-Representing_Model_Uncertainty_in_Deep_Learning_Gal_2016.pdf)

[Bayesian Learning for Neural Networks: Algorithmic Survey, Martin Margis, Alexandros Iosifidis, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bayesian_Learning_for_Neural_Networks-an_algorithmic_survey_2023.pdf)

[Bayesian Methods for Neural Networks, Joao de Freitas, Cambridge U., 2000, PhD thesis](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Bayesian_Methods_For_Neural_Networks_Freitas_PhD_thesis_CambdridgeU_2000.pdf)

[A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference, Kumar Shridhar et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Comprehensive_guide_to_Bayesian_Convolutional_Neural_Network_with_Variational_Inference_Shridhar_2019.pdf)

[Convex Bounds on the Softmax Function with Applications to Robustness Verification, Wei, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Convex_Bounds_on_the_Softmax_Function_with_Applications_to_Robustness_Verification_wei_23.pdf)

[Everything is Connected: Graph Neural Networks, Peter Velickovic, DeepMind, U Cambridge, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Everything_is_Connected-Graph_Neural_Networks_Petar_Velickovic_2023.pdf)

[A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges, Abdar, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Review_of_Uncertainty_Quantification_in_Deep_Learning-Techniques_Applications_and_Challenges_Abdar_2021.pdf)

[Solving olympiad geometry without human demonstrations, Trieu H. Trinh et al, DeepMind, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Solving_olympiad_geometry_without_human_demonstrations_Trinh_DeepMind_2023.pdf)

[HyperNetworks, David Ha, GoogleBrain, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Hypernetworks_David_Ha_GoogleBrain_2017.pdf)

[MotherNet: A Foundational Hypernetwork for Tabular Classification, A.C. Mueller et al, Microsoft Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/MotherNet-A_Foundational_Hypernetwork_for_Tabular_Classification_Mueller_2023.pdf)

[Using Sequences of Life-events to Predict Human Lives, Germans Savcisens et al, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Using_Sequences_of_Life-events_to_Predict_Human_Lives_Savcisens_2023.pdf)

[Lie Group Decompositions for Equivariant Neural Networks, Mircea Mironenco, Patrick Forre, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Lie_Group_Decompositions_for_Equivariant_Neural_Networks_Mironenco_2023.pdf)

[The Unreasonable Ineffectiveness of the Deeper Layers, A. Gromov et al, Meta FAIR, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/The_Unreasonable_Ineffectiveness_of_the_Deeper_Layers_Gromov_2024.pdf)

[Why Do We Need Weight Decay in Modern Deep Learning? Francesco D’Angelo et al, EPFL Lausanne, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Why_Do_We_Need_Weight_Decay_in_Modern_Deep_Learning_DAngello_2024.pdf)

[What Every Computer Scientist Should Know About Floating-Point Arithmetic, D. Goldberg, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/WhatEveryScientistShouldKnowAboutFloatingPointNumbersGoldberg1991.pdf)

[One Pixel Attack for Fooling Deep Neural Networks, J. Su et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/One_Pixel_Attack_for_Fooling_Deep_Neural_Networks_Su_2019.pdf)

related youtube presentation: [here](https://youtu.be/_y1tIdnB__Y?si=kZyy0ldXFUWTuqYx)

[Exploring Adversarial Examples: Patterns of One-Pixel Attack, David Kügler et al, TU Darmstadt, 2019](https://ar5iv.labs.arxiv.org/html/1806.09410)


## Sequence to Sequence and RNNs

[Recurrent Neural Networks Regularization, Wojciech Zaremba et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Recurrent_Neural_Networks_Regularization_Zaremba_2015.pdf)

[Order Matters: Sequence to Squence for Sets, Oriol Vinyals et al, Google Brain, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Order_Matters_Sequence_to_Sequence_for_Sets_Vinyals_GoogleBrain_2016.pdf)

[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy's blog, 2015](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

## Training, Fine-Tuning, Regularization of Deep Learning Models

[Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer, Greg Yang et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/fine-tunning/Tensor_Programs_V-Tuning_Large_Neural_Networks_via_Zero-Shot_Hyperparameter_Transfer_Yang_2022.pdf)

related repo: https://github.com/microsoft/mup

[Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory, Charles H. Martin, 2019](https://www.youtube.com/live/DymfJGOOK_4?si=PYRrwiCIlbqeyo1)

[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Sergey Ioffe, Christian Szegedy, Google, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/batch_normalization/Batch_Normalization-Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift_Ioffe_2015.pdf)

[Recurrent Neural Networks Regularization, Wojciech Zaremba et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Recurrent_Neural_Networks_Regularization_Zaremba_2015.pdf)


## Loss Functions and Metrics

[Loss Functions and Metrics in Deep Learning, Juan Terven et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Loss_Functions_and_Metrics_in_Deep_Learning_Terven_2024.pdf)

[softmax is not enough (for sharp out-of-distribution), Peter Velickovic et al, DeepMind, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/activation/softmax_is_not_enough_for_sharp_out-of-distribution_Velickovic_2024.pdf)

[Flat Channels to Infinity in Neural Loss Landscapes, Flavio Martinelli et al, EPFL, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Flat_Channels_to_Infinity_in_Neural_Loss_Landscapes_Martinelli_2025.pdf)

## Neural Network Optimizers, Optimization Problems and Algorithms

[An overview of gradient descent optimization algorithms, S. Ruder, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/optimizers/An_overview_of_gradient_descent_optimization_algorithms_Ruder_2017.pdf)

[Variational Stochastic Gradient Descent for Deep Neural Networks, Haotian Chen et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/optimizers/Variational_Stochastic_Gradient_Descent_for_Deep_Neural_Networks_Chen_2025.pdf)

repo: https://github.com/generativeai-tue/vsgd/tree/main

[Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models, X. Xie et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/optimizers/Adan-Adaptive_Nesterov_Momentum_Algorithm_for_Faster_Optimizing_Deep_Models_Xie_2024.pdf)

[Adam - A Method for Stochastic Optimization, D. Kingma et al, OpenAI, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/optimizers/Adam-A_Method_for_Stochastic_Optimization_Kingma_OpenAI_2017.pdf)

[Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, John Duchi et al, UC Berkeley, 2011](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/optimizers/Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization_Duchi_2011.pdf)

[Optimization for Deep Learning: Theory and Algorithms, Ruoyu Sun, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Optimization_for_deep_learning-theory_and_algorithms_Sun_2019.pdf)


## Feature Learning
[Mechanism of feature learning in deep fully connected networks
and kernel machines that recursively learn features, A. Radhakrishnan, MIT, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Mechanism_of_feature_learning_in_deep_fully_connected_networks_and_kernel_machines_that_recursively_learn_features_Radhakrishnan_2023.pdf)

[Supplementary material for Mechanism for feature learning in neural networks and backpropagation-free machine learning models, A. Radhakrishnan et al, 2024, Science](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Mechanism_for_feature_learning_in_neural_networks_and_backpropagation-free_machine_learning_models_science_supplement_2024.pdf)

[Foundations of Machine Learning: Over-parameterization and Feature Learning, A. Radhakrishnan, PhD Thesis, MIT, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/radhakrishnan-aradha-phd-eecs-2023-thesis.pdf)

## Test-Time Adaptation

[Deep Researcher with Test-Time Diffusion, Rujun Han et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/test_time_adaptation/Deep_Researcher_with_Test-Time_Diffusion_Han_2025.pdf)

[Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment, J. Guo et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/test_time_adaptation/Everything_to_the_Synthetic-Diffusion-driven_Test-time_Adaptation_via_Synthetic-Domain_Alignment_Guo_2024.pdf)

related repo: https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment

## Compression

[Learning to Compress: Local Rank and Information Compression in Deep Neural Networks, Niket Patel, Ravid Shwartz Ziv, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/compression/Learning_to_Compress-Local_Rank_and_Information_Compression_in_Deep_Neural_Networks_Patel_Ziv_2024.pdf)

## Generalization in Deep Learning Netoworks, the Double Descent Phenomenon, and Statistical Mechanics interpretations

[A Geometric Modeling of Occam’s Razor in Deep Learning, Ke Sun, Frank Nielsen, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/geometric_deep_learning/A_Geometric_Modeling_of_Occams_Razor_in_Deep_Learning_Sun_2025.pdf)

[Understanding the Double Descent Phenomenon in DeepLearning, Marc Lafon, Alexandre Thomas, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Understanding_the_Double_Descent_Phenomenon_in_Deep_Learning_Lafon_2021.pdf)

[Reconciling modern machine learning practice and the bias-variance trade-of, Mikhail Belkin et al, The OSU, 2019 ](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Reconciling_modern_machine_learning_practice_and_the_bias-variance_trade-off_Belkin_2019.pdf)

[On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization, S. Arora et al, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/On_the_Optimization_of_Deep_Networks-Implicit_Acceleration_by_Overparameterization_Arora_2018.pdf)

[Relational inductive biases, deep learning, and graph networks, Peter W Battaglia et al, DeepMind, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Relational_inductive_biases_deep_learning_and_graph_networks_Bataglia_2018.pdf)

[Rethinking–or Remembering–Generalization in Neural Networks, Charles H Martin, April 2018](https://calculatedcontent.com/2018/04/01/rethinking-or-remembering-generalization-in-neural-networks/)

[Rademacher Complexity & VC Dimension, Jordan Boyd-Graber, 2025](https://www.youtube.com/watch?v=gR9Q8pS03ZE)

[Foundations: The Partition Function, Charles H. Martin, 2013](https://calculatedcontent.com/2013/11/14/foundations-the-parition-function/)

[Measuring the VC dimension of a Learning Machine, V. Vapnik et al, 1994](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Measuring_the_VC-Dimension_of_a_Learning_Machine_Vapnik_LeCunn_1994.pdf)

[Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior, Charles H. Martin et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Rethinking_generalization_requires_revisiting_old_ideas-statistical_mechanics_approaches_and_complex_learning_behavior_Martin_2019.pdf)

[Understanding Deep Learning Requires Rethinking Generalization, C. Zhang et al, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/double_descent/Understanding_deep_learning_requires_rethinking_generalization_Zhang_2016.pdf)

[Understanding deep learning requires rethinking generalization, Chiyuan Zhang et al, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/generalization/Understanding_deep_learning_requires_rethinking_generalization_Zhang_2018.pdf)

[Measuring VC Dimension of Learning Machine, Vladimir Vapnik et al, 1994](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/generalization/Measuring_VC_dimension_of_learning_machine_vapnik-levin-lecun-94.pdf)

[This is why large language models can understand the world, Algorithmic Simplicity, April 2025](https://youtu.be/UKcWu1l_UNw?si=D_ER9RmSNCpOqSPe)

[Generative Modeling of Weights: Generalization or Memorization? Boya Zeng et al, Princeton U., 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/generalization/Generative_Modeling_of_Weights-Generalization_or_Memorization_Zheng_Princeton_2025.pdf)

github repo: https://github.com/boyazeng/weight_memorization

## Inductive Bias in Deep Learning Networks

[Hints, Yaser-Abu Mostafa, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/inductive_bias/Hints_Yaser_Abu_Moustafa_1995.pdf)

[The Lack of Apriori Distinctions Between Learning Algorithms, David H. Wolpert, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/inductive_bias/the-lack-of-a-priori-distinctions-between-learning-Wolpert_1996.pdf)

[The Role of Occam's Razor in Knowledge Discovery, Pedro Domingos, 1999](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/inductive_bias/The_Role_of_Occams_Razor_in_Knowledge_Discovery_Domingos_1999.pdf)

[Ockham’s Razor, Truth, and Information, Kevin T. Kelly, CMU, 2008](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/inductive_bias/Ockhams_Razor_Truth_and_Information_Kelly_2008.pdf)


## Deconstructing Deep Neural Networks

[The Geometry of Concepts: Sparse Autoencoder Feature Structure, Y. Li et al (Max Tegmark's group at MIT), 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/The_Geometry_of_Concepts_Sparse_Autoencoder_Feature_Structure_Li_2024.pdf)

[Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Layer_by_Layer-Uncovering_Hidden_Representations_in_Language_Models_Skean_2025.pdf)

[A Geometric Modeling of Occam’s Razor in Deep Learning, Ke Sun, Frank Nielsen, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/geometric_deep_learning/A_Geometric_Modeling_of_Occams_Razor_in_Deep_Learning_Sun_2025.pdf)


## Grokking

[The Complexity Dynamics of Grokking, Branton DeMoss, Silvia Sapora, Jakob Foerster, Nick Hawes, Ingmar Posner, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/grokking/The_Complexity_Dynamics_of_Grokking_DeMoss_2024.pdf)

[Towards Understanding Grokking: An Effective Theory of Representation Learning, Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, Mike Williams, MIT, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/grokking/Towards_Understanding_Grokking-An_Effective_Theory_of_Representation_Learning_Liu_MIT_2022.pdf)

[Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/grokking/Grokking-Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets_Power_2022.pdf)

[Grokking, a New Form of Reasoning, Ignacio de Gregorio, Medium, 2024](https://medium.com/@ignacio.de.gregorio.noblejas/grokking-a-new-form-of-reasoning-6785ea89d2ec)

[Grokking: A Deep Dive into Delayed Generalization in Neural Networks, Kirouane Ayoub, Medium, 2024](https://medium.com/@ayoubkirouane3/grokking-a-deep-dive-into-delayed-generalization-in-neural-networks-e117fdef07a1)

[How to explain grokking, S.V. Kozyrev, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/grokking/How_to_explain_grokking_Kozyrev_2025.pdf)

## Emergence phenomena

[Intuitive physics understanding emerges from self-supervised pretraining on natural videos, Q. Garrido et al, Meta FAIR, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/emergence/Intuitive_physics_understanding_emerges_from_self-supervised_pretraining_on_natural_videos_Garrido_2025.pdf)

[From Colors to Classes: Emergence of Concepts in Vision Transformers, Teresa Dorszewski et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/emergence/From_Colors_to_Classes-Emergence_of_Concepts_in_Vision_Transformers_Dorszewski_2025.pdf)

[Leveraging chaos in the training of artificial neural networks, Pedro Jim´enez-Gonzalez, Miguel C. Soriano and Lucas Lacasa, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/emergence/Leveraging_chaos_in_the_training_of_artificial_neural_networks_Gonzalez_2025.pdf)

[Flat Channels to Infinity in Neural Loss Landscapes, Flavio Martinelli et al, EPFL, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Flat_Channels_to_Infinity_in_Neural_Loss_Landscapes_Martinelli_2025.pdf)

## AutoEncoders

[Autoencoders, Dor Bank, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoders.pdf)

[Autoencoders, Unsupervised Learning, and Deep Architectures, Pierre Baldi, 2012](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Autoencoders_Unsupervised_Learning_and_Deep_Architectures_Baldi_2012a.pdf)

[Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima, Pierre Baldi, Kurt Hornik, 1988](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Neural_Networks_and_Principal_Component_Analysis-Learning_from_Examples_Without_Local_Minima_Baldi_Hornik-89.pdf)

[Neural Networks: Unleashing the Power of Latent Space Compression, Julien Pascal, Medium, 2023](https://medium.com/@julien.pascal/neural-networks-unleashing-the-power-of-latent-space-compression-2c8630f6f6cc)

[The Sparse Autoencoder, Andrew Ng, Lecture Notes CS294A](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/sparseAutoencoder_AndrewNg_LectureNotes.pdf)

[Autoencoders: An Ultimate Guide for Data Scientists, Niklas Lang, Towards Data Science, 2024](https://towardsdatascience.com/autoencoders-an-ultimate-guide-for-data-scientists-dca3e56a070e)

## Attention

[Bidirectional Recurrent Neural Networks, Mike Schuster, Kuldip Paliwal, IEEE Transactions on Signal Processing,1997](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Bidirectional_Recurrent_Neural_Networks_Schuster_Paliwal_1997.pdf)

[Neural Networks for Pattern Recognition, Christopher M. Bishop, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Neural_Networks_for_Pattern_Recognition-Bishop_1995.pdf)

[Recurrent Continuous Translation Models, Nal Kalchbrenner, Phil Blunsom, Oxford U.,2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Recurrent_continuous_translation_models_Kalchbrenner_Blunsson_OxfordU_2013.pdf)

[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, K. Cho, B. van Merrienboer, C. Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_Cho_UdMontreal_2014.pdf)

[Generating Sequences With Recurrent Neural Networks, Alex Graves, U of Toronto, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Generating_Sequences_With_Recurrent_Neural_Networks_Graves_2014.pdf)

[End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results, Jan Chorowski et al, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/End-to-end_Continuous_Speech_Recognition_using_Attention-based_Recurrent_NN-First_Results_Chorowski_2014.pdf)

[Translation Modeling with Bidirectional Recurrent Neural Networks, M. Sundermeyer et al, Aachen U., 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Translation_Modeling_with_Bidirectional_Recurrent_Neural_Networks_Sundermeyer_2014.pdf)

[LSTM Can Solve Hard Long Time Lag Problems, Sepp Hochreiter, Jurgen Schmidhuber NIPS, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1996-lstm-can-solve-hard-long-time-lag-problems-Paper_Hochreiter.pdf)

[Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks, J. Schmidhuber et al, 1992](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Learning_to_Control_Fast-Weight_Memories_1992-schmidhuber.pdf)

[Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation, Jean Pouget-Abadie et al, EPF, 2014](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Overcoming_the_Curse_of_Sentence_Length_for_Neural_Machine_Translation_using_Automatic_Segmentation_Puget-Abadie_EPF_2014.pdf)

[Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, E. Kiperwasser, Y. Goldberg,  2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simple_and_Accurate_Dependency_Parsing_Using_Bidirectional_LSTM_Feature_Representations_Kiperwasser_2016.pdf)

[Contextual Position Encoding: Learning to Count What's Important, O. Golovneva et al, Meta, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Contextual_Position_Encoding-Learning_to_Count_Whats_Important_Golovneva_Meta_2024.pdf)

[On The Relationship between Self-Attention and Convolutional Layers, JB Cordonnier et al, ICLR, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/On_the_Relationship_between_Self-Attention_and_Convolutional_Layers_Cordonnier_ICLR_2020.pdf)

[Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation, Srinadh Bhojanapalli et al, Google Research, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Eigen_Analysis_of_Self-Attention_and_its_Reconstruction_from_Partial_Computation_Bhojanapali_2021.pdf)

[The FFTStrikes Back: An Efficient Alternative to Self-Attention, Jacob Fein-Ashley, U of SoCal, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/The_FFT_Strikes_Back-An_Efficient_Alternative_to_Self-Attention_Fein-Ashley_2025.pdf)

[Hypergraph Convolution and Hypergraph Attention, Song Bai et al, U. of Oxford, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/graph_networks/Hypergraph_Convolution_and_Hypergraph_Attention_Bai_2020.pdf)

[Log-Linear Attention, Han Guo et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Log-Linear_Attention_Guo_2025.pdf)

[Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport, E. Littman, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/Scaled-Dot-Product_Attention_as_One-Sided_Entropic_Optimal_Transport_Littman_2025.pdf)

[All You Need To Know About The Self-Attention Layer, Damien Benveniste, The AiEdge, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/All_You_Need_To_Know_About_The_Self-Attention_Layer_Beneveniste_2025.pdf)

[The Mathematics of Causality, Miquel Noguer i Alonso, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/attention/The_Mathematics_of_Causality_Noguer_i_Alonso_2025.pdf)

## Embeddings

[Embeddings in Natural Language Processing: Theory and Advances in Vector Representation of Meaning, M. Pilhevar, J. Camacho-Collados, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Embeddings_in_NLP_Pilhevar_book_draft.pdf)

[Vector Semantics and Embeddings, Daniel Jurafsky and James H Martin, Speech and Language Processing, Chapter 6, Stanford U,, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Vector_Semantics_and_Embeddings_Jurafsky_2024.pdf)

[Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov et al, Google, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Efficient_Estimation_of_Word_Representations_in_Vector_Space_Mikolov_Google_2013.pdf)

[Distributed Representations of Words and Phrases and their Compositionality, Thomas Mikolov et al, Google, 2013](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality_Mikolov_2013.pdf)

[Factors Influencing the Surprising Instability of Word Embeddings, L. Wendtlandt et al, U. Michigan Ann Arbor, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Factors_Influencing_the_Surprising_Instability_of_Word_Embeddings_Wendlandt_AnnArbor_2018.pdf)

[Is Cosine-Similarity of Embeddings Really About Similarity? Harald Steck, Chaitanya Ekanadham, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Is_Cosine-Similarity_of_Embeddings_Really_About_Similarity_Steck_2024.pdf)

[Word Embeddings What Works, What Doesn't And How To Tell The Difference For Applied Research, A. Spirling, P. Rodriguez, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Word_Embeddings_Spirling_Rogriguez_2019.pdf)

[Word Embeddings - A Survey, Felipe Almeida, Geraldo Xexeo, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/embeddings/Word_Embeddings-A_Survey_Almeida_2023.pdf)

## LLM insights

[Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/llm_insights/Layer_by_Layer-Uncovering_Hidden_Representations_in_Language_Models_Skean_2025.pdf)

## Mixture of Experts

[The First Sparse Mixture of Experts, D. Benveniste, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/The_first_sparse_mixture_of_experts_Bienveniste_2025.pdf)

[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, N. Shazeer et al, 2017](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/Outrageously_Large_Neural_Networks-The_Sparsely-Gated_Mixture-of-Experts_Layer_Shazeer_2017.pdf)

[The Evolution of Mixture of Experts: A Survey from Basics to Breakthroughs, A. Vats et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/The_Evolution_of_Mixture_of_Experts-A_Survey_from_Basics_to_Breakthroughs_Vats_2024.pdf)

[A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications, S. Mu et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/Comprehensive_Survey_of_Mixture-of-Experts-Algorithms_Theory_and_Applications_Mu_2025.pdf)

[A Survey on Mixture of Experts in Large Language Models, W. Cai et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/Survey_on_Mixture_of_Experts_in_Large_Language_Models_Cai_2025.pdf)

[Mixture of Experts, Lecture from 10-423/10-623 Generative AI, Matt Gromley, Henry Chai](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/Mixture_of_Experts_Gormley_2024.pdf)

[Adaptive Mixture of Local Experts, R.A. Jacobs et al, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/Adaptive_Mixture_of_Local_Experts_Jacobs_1991.pdf)

[HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts, H. Zhao et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/mixture_of_experts/HyperMoE-Towards_Better_Mixture_of_Experts_via_Transferring_Among_Experts_Zhao_2024.pdf)


## Self-organizing maps

[The Self-Organizing Map, Kohonen, IEEE, 1998](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheSelfOrganizing_map_Kohonen_IEEE_1998.pdf)

[Kohonen Self-Organizing Map for the Traveling Salesperson Problem, Lucas Brocki, 2010](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Kohonen-Self-Organizing_Map_for_the_Traveling_Salesperson_Problem_Brocki_2010.pdf)

[Using Self-Organizing Maps to solve the Traveling Salesman Problem, Diego Vincente's blog](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Using_Self-Organizing_Maps_to_solve_the_Traveling_Salesman_Problem_Diego_Vicente_Blog.pdf)

[Diego Vincente's github repo on self-organizing maps for solving TSP](https://github.com/diego-vicente/som-tsp)

## State Space Models

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu, T. Dao, CMU, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2023.pdf)

[Mamba: Can it replace Transformers? Vishal Rajput, Medium Jan 8, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba_Can_it_replace_Transformers_Vishal_Rajput_Jan_2024_Medium.pdf)

[Why Mamba was rejected? Joe El Khoury, Medium, Feb 28, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Why_Mamba_was_rejected_recently_at_the_ICLR_by_Joe_Elkhoury_Feb_2024_Medium.pdf)

[A Visual Guide to Mamba and State Space Models, Maarten Grootendorst, 2024](https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6)

[Mamba Explained, Kola Ayonrinde, The Gradient, March 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Mamba_Explained_The_Gradient.pdf)

[HiPPO: Recurrent Memory with Optimal Polynomial Projections, A. Gu et al, Stanford U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/HiPPO-Recurrent_Memory_with_Optimal_Polynomial_Projections_Gu_2020.pdf)

[How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections, A. Gu et al, Stanford U., 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/How_to_Train_Your_HiPPO-State_Space_Models_with_Generalized_Orthogonal_Basis_Projections_Gu_2022.pdf)

[Hungry Hungry Hippos: Towards Language Modeling with State Space Models, D. Fu, T. Dao, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Hungry_Hungry_Hippos-Towards_Language_Modeling_with_State_Space_Models_Fu_Dao_Stanford_2023.pdf)

[Efficiently Modeling Long Sequences with Structured State Spaces, K. Goel, A. Gu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Efficiently_Modeling_Long_Sequences_with_Structured_State_Spaces_Gu_Stanford_2022.pdf)

[Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers, A. Gu et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Combining_Recurrent_Convolutional_and_Continuous-time_Models_with_Linear_State-Space_Layers_Gu_2021.pdf)

[Diagonal State Spaces are as Effective as Structured State Spaces, A. Gupta, A. Gu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/state_space_models/Diagonal_State_Spaces_are_as_Effective_as_Structured_State_Spaces_Gupta_Gu_2022.pdf)

## Graph Neural Networks

[Graph Neural Networks: Foundations, Frontiers, and Applications, Lingfei Wu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/GraphNeuralNetworks-FoundationsFrontiers_and_Applications_Wu_2022.pdf)

[Introduction to Graph Neural Networks, Zhiyuan Liu et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Intro_to_Graph_Neural_Networks_Liu_2022.pdf)

[Deep Learning on Graphs, Yao Ma et al, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/DeepLearning_on_Graphs_Ma_2020.pdf)

[Graph Representation Learning, W. Hamilton, McGill U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Graph_Representation_Learning_Book_Hamilton_2020.pdf)

[DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs, D. Zheng et al, AWS AI, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DistDGL-Distributed_Graph_Neural_Network_Training_for_Billion-Scale_Graphs_Zheng_AWS_AI_2023.pdf)

[Temporal Graph Learning in 2024: continue the journey for evolving networks, S. Huang, Towards Data Science, 2024](https://towardsdatascience.com/temporal-graph-learning-in-2024-feaa9371b8e2)

[TGL: A General Framework for Temporal GNN Training on Billion-Scale Graphs, H. Zhou, U o SoCal, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/tgl-a-general-framework-for-temporal-gnn-training-on-billion-scale-graphs-scalable-data-science.pdf)

[Graph neural networks in TensorFlow, Dustin Zelle, Distin Zelle, Feb 6, 2024, online blog](https://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html)

## Long Short Term Memory (LSTM)

[xLSTM: Extended Long Short-Term Memory, Maximilian Beck et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/xLSTM-Extended_Long_Short-Term_Memory_Beck_2024.pdf)

[Long Short Term Memory, Sepp Hochreiter, 1997](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/LongShortTermMemory.pdf)

[LSTM Can Solve Hard Long Time Lag Problems, Sepp Hochreiter, Jurgen Schmidhuber NIPS, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NIPS-1996-lstm-can-solve-hard-long-time-lag-problems-Paper_Hochreiter.pdf)

[Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, E. Kiperwasser, Y. Goldberg,  2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simple_and_Accurate_Dependency_Parsing_Using_Bidirectional_LSTM_Feature_Representations_Kiperwasser_2016.pdf)

[Understanding LSTM: a Tutorial into Long Short Term Memory Recurrent Neural Networks, Ralf C. Staudemeyer et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TutorialOnLongShortTermMemory2019.pdf)

[Understanding LSTM networks, Colah's blog, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/UnderstandingLSTMNetworks-colahsblog.pdf)

[Fundamentals of RNN and LSTM Network, Alex Sherstinsky, MIT, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/FundamentalsOfRNNandLSTMNetwork.pdf)

## Representation Learning

[I-Con: A Unifying Framework for Representation Learning, Shaden Alshammari et al, MIT, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/representation_learning/I-Con-A_Unifying_Framework_for_Representation_Learning_Alshammari_MIT_2025.pdf)

   I-Con blog: https://mhamilton.net/icon

[Harnessing the Universal Geometry of Embeddings, Rishi Jha et al, Cornel U., 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/representation_learning/Harnessing_the_Universal_Geometry_of_Embeddings_Jha_2025.pdf)

[The Platonic Representation Hypothesis, M. Huh et al, MIT, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/representation_learning/The_Platonic_Representation_Hypothesis_Huh_MIT_2024.pdf)

[Occam's Razor for Self-Supervised Learning: What Is Sufficient to Learn Good Representation?, M. Ibrahim et al, FAIR Meta, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/representation_learning/Occams_Razor_for_Self-Supervised_Learning-What_is_Sufficient_to_Learn_Good_Representations_Ibrahim_2024.pdf)

[Cross-Entropy is All You Need To Invert The Data Generating Process, P. Reizinger et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/representation_learning/Cross-Entropy_is_All_You_Need_to_Invert_the_Data_Generating_Process_Reizinger_2024.pdf)

[Towards Understanding Grokking: An Effective Theory of Representation Learning, Ziming Liu, Ouail Kitouni, Niklas Nolte, Eric J. Michaud, Max Tegmark, Mike Williams, MIT, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/grokking/Towards_Understanding_Grokking-An_Effective_Theory_of_Representation_Learning_Liu_MIT_2022.pdf)

[Graph Representation Learning, W. Hamilton, McGill U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Graph_Representation_Learning_Book_Hamilton_2020.pdf)


## Gated Recurrent Units (GRU)

[A Dynamic Regime-Switching Model Using Gated Recurrent Straight-Through Units, N. Antulov-Fantulin, A. Cauderan, Petter N. Kolm, NYU Courant Institute, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/gated_recurrent_unit/A_Dynamic_Regime-Switching_Model_Using_Gated_Recurrent_Straight-Through_Units_Fantoulin_Cauderas_Kolm_2024.pdf)

## Liquid Time-constant Networks 

[Liquid Time-constant Networks, R. Hasani et al, MIT, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/liquid_networks/Liquid_Time-constant_Networks_Hasani_2020.pdf)

## Statistical Mechanics of Learning

[Statistical Mechanics of Deep Learning Neural Networks - The Back-Propagating Kernel Renormalization, Q. Li, H. Sompolonsky, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Statistical_Mechanics_of_Deep_Linear_Neural_Networks-The_Back-Propagating_Kernel_Renormalization_Li_Sompolinsky_2021.pdf)

[Towards a new Theory of Learning: Statistical Mechanics of Deep Neural Networks, 2019, Charles H Martin](https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/)

[Visualizing the Loss Landscape of Neural Nets, H. Li et al, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Visualizing_the_Loss_Landscape_of_Neural_Nets_Li_2018.pdf)

[Why does Deep Learning work?, Charles H Martin, 2015](https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/)

[Qualitatively Characterizing Neural Network Optimization Problems, Ian Goodfellow et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Qualitatively_Characterizing_Neural_Network_Optimization_Problems_Goodfellow_2015.pdf)

[The Loss Surfaces of Multilayer Networks, A. Choromanska et al, 2015](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/The_Loss_Surfaces_of_Multilayer_Networks_Choromanska_2015.pdf)

[Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation, Tal Zeevi et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Rate-In-Information-Driven_Adaptive_Dropout_Rates_for_Improved_Inference-Time_Uncertainty_Estimation_Zeevi_2024.pdf)

[A Common Logic to Seeing Cats and Cosmos, Quanta Magazine, Natalie Wolchover, 2014](https://www.quantamagazine.org/a-common-logic-to-seeing-cats-and-cosmos-20141204/)

[Mimicking The Folding Pathway to Improve Homology-Free Protein Structure Prediction, J. DeBartolo, 2008](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/debartolo-et-al-2009-mimicking-the-folding-pathway-to-improve-homology-free-protein-structure-prediction.pdf)

[Funnels in Energy Landscapes, K. Klemm et al, 2007](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Funnels_in_Energy_Landscapes_Klemm_2007.pdf)

[Understanding Protein Folding with Energy Landscape Theory, K. Plotkin, JN Onuchic, 2002](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Understanding_protein_folding_with_energy_landscape_theory_PlotkinOnuchic_part1.pdf)

[Landscape Statistics of low autocorrelated binary string problem, F. Ferreira et al, 2000](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Landscape_statistics_of_the_low_autocorrelated_binary_string_problem_Ferreira_2000.pdf)

[Energy Landscapes, Supergraphs, and "Folding Funnels" in Spin Systems, Piotr Garstecki et al, 1999](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Energy_Landscapes_Supergraphs_and_Folding_Funnels_in_Spin_Systems_Garstecki_1999.pdf)

[From Levinthal to pathways to funnels, K. Dill, HS Chan, 1997](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/From_Levintal_to_pathways_to_funnels_Dill_1997.pdf)

[Statistical Mechanics of Learning from Examples, HS Seung, H. Sompolinsky, 1994](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/Statistical_mechanics_of_learning_from_examples_1992-seung-somplinsky.pdf)

[Spin Glasses and the Statistical Mechanics of Protein Folding, JD Bryngelson, PG Wolynes, 1987](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/statistical_mechanics_of_DL/bryngelson-wolynes-1987-spin-glasses-and-the-statistical-mechanics-of-protein-folding.pdf)

### SETOL: A Semi-Empirical Theory of (Deep) Learning

[SETOL: A Semi-Empirical Theory of (Deep) Learning, Charles H Martin, Christopher Hinrichs, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/SETOL/SETOL-A_Semi-Empirical_Theory_of_Deep_Learning_Martin_2025.pdf)

## Geometric Deep Learning

[Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges, M. Bronstein et al, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Geometric_Deep_Learning_Grids_Groups_Graphs_Geodesics_and_Gauges_Bronstein_2021.pdf)

[Equivariant non-linear maps for neural networks on homogeneous spaces, E. Nyholm et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/geometric_deep_learning/Equivariant_non-linear_maps_for_neural_networks_on_homogeneous_spaces_Nyholm_2025.pdf)

## Topological Data Analysis 
[Torsion in Persistent Homology and Neural Networks, Maria Walch, Heidelberg U., 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/torsion/Torsion_in_Persistent_Homology_and_Neural_Networks_Walch_2025.pdf)

## Topos Theory in Deep Learning

[Relative Toposes for Artifical General Intelligence, part 1, O. Caramello, June 26th, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/topos_theory/RelativeToposesAGI_part1.pdf)

[Relative Toposes for Artifical General Intelligence, part 2, O. Caramello, July 3rd, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/topos_theory/RelativeToposesAGI_part2.pdf)

## Imitation Learning

[X-IL: Exploring the Design Space of Imitation Learning Policies, X. Jia et al, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/X-IL-Exploring_the_Design_Space_of_Imitation_Learning_Policies_Jia_2025.pdf)

[Visual Imitation Made Easy, Sarah Young et al, Berkeley, CMU, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/Visual_Imitation_Made_Easy_Young_2021.pdf)

[A Survey of Robot Learning from Demonstration, BD Argall et al, CMU, 2009](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/A_survey_of_robot_learning_from_demonstration_Argail_2009.pdf)

[An Algorithmic Perspective on Imitation Learning, T. Osa et al, U of Tokyo, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/An_Algorithmic_Perspective_on_Imitation_Learning_Osa_2018.pdf)

[Behavioral Cloning from Observation, F. Torabi et al, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/Behavioral_Cloning_from_Observation_Torabi_2018.pdf)

[Learning Latent Plans from Play, C. Lynch et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/Learning_Latent_Plans_from_Play_Lynch_2019.pdf)

[Learning from Demonstration, Stefan Schaal, 1996](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/NIPS-1996-learning-from-demonstration-Paper.pdf)

[Alvinn: An Autonomous Land Vehicle in a Neural Network, DA Pomerleau, NIPS, 1988](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/imitation_learning/NIPS-1988-alvinn-an-autonomous-land-vehicle-in-a-neural-network-Paper.pdf)

## Equivalence theorems

[The Mathematical Equivalence between Decision Trees and Artificial Neural Networks, Miquel Noguer i Alonso, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/equivalence_theorems/Mathemaical_Equivalence_between_DecisionTrees_and_ANN_Allonco_2024.pdf)

## Theorem Proving
[NeurIPS Tutorial on Machine Learning for Theorem Proving, video](https://machine-learning-for-theorem-proving.github.io/)

[DeepMath - Deep Sequence Models for Premise Selection, Alexander Alemi, Francois Chollet et al, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/theorem_proving/DeepMath-Deep_Sequence_Models_for_Premise_Selection_alemi_2016.pdf)

## Computability of Neural Networks

[Turing Computability with Neural Nets, Hava T. Siegelmann, Eduardo Sontag, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/TuringComputabilityWithNeuralNets_Siegelman1991.pdf)

[On the computational power of Neural Nets, Hava T. Siegelmann, 1992 (earlier version)](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/OnTheComputationalPowerOfNeuralNets_1992_Siegelmann.pdf)

[On The Computation Power of Neural Nets, Hava T. Siegelmann, Eduardo Sontag, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/OnTheComputationalPowerOfNeuralNets_1995_Siegelmann_JComSysSci.pdf)

[Computation beyond Turing limit, Hava T. Siegelmann, 1995](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/ComputationBeyondtheTuringLimit_1995_Siegelmann_Science.pdf)

[Neural Networks and Analog Computation beyond Turing limit, 1999](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/NeuralNetworksandAnalogComputationBeyondTheTuringLimit.pdf)

[Efficient Simulation of Finite Automata by Neural Nets, Noga Alon et al, JACM, Vol. 38, No. 2, 1991](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/computability/Efficient_simulations_of_finite_automata_by_neural_nets_Alon_1991.pdf)

## Algorithmic Aspects and Parallelization of Neural Networks

[Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks, Z. Jia et al, ICML, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/algorithmic_aspects/Exploring_the_hidden_dimension_in_accelerating_convolutional_neural_networks_Jia_ICML_2018.pdf)

## Protein Folding and AlphaFold

[AlphaFold 3 predicts the structure and interactions of all of life’s molecules, Google blog, May 08, 2024](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#future-cell-biology)

FastFold github: https://github.com/hpcaitech/fastfold

Prediction-Powered Inference github: https://github.com/aangelopoulos/ppi_py

Protein Workshop github: https://github.com/a-r-j/proteinworkshop

## Antibody Discovery 

[Chai-Discovery: Zero-shot antibody design in a 24-well plate, Chai Discovery Team, 2025](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/chai_discovery/Chai-Discovery_Zero-shot_antibody_design_in_a_24-well_plate_technical_report_Chai_discovery_team_2025.pdf)

## Deep Learning for Image Processing

Resources on this [link](https://github.com/dimitarpg13/deep_learning_for_image_processing/blob/main/Resources.md)


## Deep Learning for Physical Applications

[Predicting disruptive instabilities in controlled fusion plasmas through deep learning, Julian Kates-Harbeck et al, Harvard U, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Predicting_disruptive_instabilities_in_controlled_fusion_plasmas_through_deep_learning_Kates-Harbeck_Harvard_2019.pdf)

[Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning, Jonas Degrave, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Magnetic_control_of_tokamak_plasmas_through_deep_reinforcement_learning_Degrave_DeepMind_2019.pdf)

[First application of data assimilation-based control to fusion plasma, Yuya Morishita et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/First_application_of_data_assimilation_based_control_to_fusion_plasma_Morishita_2024.pdf)

[First Nuclear Plasma Control with Digital Twin, Sabine Hossenfelder, Feb 2024, youtube video](https://youtu.be/4VD_DLPQJBU)

[Avoiding fusion plasma tearing instability with deep reinforcement learning, Jaemin Seo et al, Nature, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/plasma_control/Avoiding_fusion_plasma_tearing_instability_with_deep_reinforcement_learning_Seo_2024.pdf)

## Time Series Forecasting

[Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series, V. Ekambaram et al, IBM, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Tiny_Time_Mixers-Fast_Pre-trained_Models_or_Enhanced_Zero_Few-Shot_Forecasting_of_Multivariate_Time_Series_Ekambaram_2024.pdf)

[A decoder-only foundation model for time-series forecasting, A. Das et al, Google, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/A_decoder-only_foundation_model_for_time-series_forecasting_Das_Google_April_2024_Preprint.pdf)

[MLP-Mixer: An all-MLP Architecture for Vision, Ilya Tolstikhin et al, Google, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/MLP-Mixer-An_all-MLP_Architecture_for_Vision_Tolstikhin_Google_2021.pdf)

[TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting, V. Ekambaram et al, IBM, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/TSMixer-Lightweight_MLP-Mixer_Model_for_Multivariate_Time_Series_Forecasting_Ekambaram_IBM_2023.pdf)

[TSMixer: An All-MLP Architecture for Time Series Forecasting, SA Chen et al, Google, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/TSMixer-An_All-MLP_Architecture_for_Time_Series_Forecasting_Chen_Google_2023.pdf)

[Predicting Venetian Lagoon Tide Levels with Multivariate Time Series Modeling, David Proietti, 2024](https://medium.com/@david.proietti_17/predicting-venetian-lagoon-tide-levels-with-multivariate-time-series-modeling-8bafdf229588)

[iTransformer: The Latest Breakthrough in Time Series Forecasting, Marco Peixeiro, Towards Data Science, April 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/iTransformer-Inverted_Transformers_Are_Effective_for_Time_Series_Forecasting_Liu_2023.pdf)

[Unified Training of Universal Time Series Forecasting Transformers, Woo, G et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Unified_Training_of_Universal_Time_Series_Forecasting_Transformers_Woo_2024.pdf)

[Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)

[How to Effectively Forecast Time Series with Amazon's New Time Series Forecasting Model, Eivind Kjosbakken, April 9, 2024, Towards Data Science](https://towardsdatascience.com/how-to-effectively-forecast-time-series-with-amazons-new-time-series-forecasting-model-9e04d4ccf67e)

[N-BEATS — The First Interpretable Deep Learning Model That Worked for Time Series Forecasting, Jonte Dancker, 2024](https://towardsdatascience.com/n-beats-the-first-interpretable-deep-learning-model-that-worked-for-time-series-forecasting-06920daadac2)

[N-BEATS: Neural basis expansion analysis for interpretable time series forecasting, BN Oreshkin et al, Element AI, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/time_series_forecasting/N-BEATS-Neural_basis_expansion_analysis_for_interpretable_time_series_forecasting_Oreshkin_ElementAI_2020.pdf)

## Visualization and Explainability

[Visualizing Stochastic Regularization for Entity Embeddings, Valerie Carey, 2024, Medium](https://towardsdatascience.com/visualizing-stochastic-regularization-for-entity-embeddings-c0109ced4a3a)

## medium
[The Math Behind the Adam Optimizer with Cristian Leo, 2024](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b)

[Ensemble Learning: Bagging and Boosting with Jonas Dieckmann](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0)

[Pruning Neural Networks, Rohit Bandaru, 2020](https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9)

[Nesterov Momentum Explained with examples in TensorFlow and PyTorch with Giorgio Martinez, 2022](https://medium.com/@giorgio.martinez1926/nesterov-momentum-explained-with-examples-in-tensorflow-and-pytorch-4673dbf21998)

[An Overview for Model Compression Techniques for Deep Learning in Space with Hannah Peterson](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5)

[Gentle Introduction to Bayesian Deep Learning with François Porcher](https://pub.aimind.so/a-gentle-introduction-to-bayesian-deep-learning-d298c7243fd6)

[Why Deep Learning Ensembles Outperform Bayesian Neural Networks with Devansh](https://medium.com/swlh/why-deep-learning-ensembles-outperform-bayesian-neural-networks-dba2cd34da24)

[Bayesian Neural Network Series Post 1: Need for Bayesian Neural Networks with Kumar Shridhar](https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2)

[Bayesian Neural Network Series Post 2: Background Knowledge with Kumar Shridhar](https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43)

[Spectral Graph Convolutions with Jose Luis Castro Garica](https://medium.com/@jlcastrog99/spectral-graph-convolutions-c7241af4d8e2)

[TS2Vec review: Towards universal time series representation learning with hierarchical contrasting with Alexander Li](https://medium.com/@findalexli/ts2vec-review-towards-universial-time-series-representation-learning-with-hierarchical-contra-c50dd789e85c)

[Is PyTorch's Nesterov Momentum Implementation Worng? with Jason Vega](https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008)

[Effective Load Balancing with Ray on Amazon SageMaker with Chaim Rand](https://towardsdatascience.com/effective-load-balancing-with-ray-on-amazon-sagemaker-d3b9020679d3)

[Layerwise learning for Quantum Neural Networks with Qiskit with Gopal Dahale](https://medium.com/qiskit/layerwise-learning-for-quantum-neural-networks-with-qiskit-e17ff4b1c419)

[Forward-Forward Algorithm: Will it replace Backpropagation? with Vishal Rajput](https://medium.com/aiguys/forward-forward-algorithm-will-it-replace-backpropagation-8d9047192975)

[Tabula Rasa: Why Do Tree-Based Algorithms Outperform Neural Networks with Salvatore Raieli](https://levelup.gitconnected.com/tabula-rasa-why-do-tree-based-algorithms-outperform-neural-networks-db641862859b)

[DeepMind Decodes the Puzzle of ‘ Grokking ’ In Neural Network Generalization Through Circuit Efficiency](https://medium.com/syncedreview/deepmind-decodes-the-puzzle-of-grokking-in-neural-network-generalization-through-circuit-b5bc32380557)

   related paper: [Explaining Grokking Through Circuit Efficiency, Varma et al, DeepMind, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Explaining_grokking_through_circuit_efficiency_Varma_DeepMind_2023.pdf)

[Liquid Neural Network : A adaptive way to train ML model with Ved Prakash](https://ved933409.medium.com/liquid-neural-network-a-adaptive-way-to-train-ml-model-b6922d755066)


[Understanding Zero-Shot Learning — Making ML More Human with Ekin Tiu](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab)

 related paper: [Learning Transferable Visual Models From Natural Language Supervision, Alec Radford et al, OpenAI, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision_Radford_2021.pdf)

 related paper: [Simulation to Scaled City: Zero-Shot Policy Transfer for Traffic Control via Autonomous Vehicles, Kathy Jang, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Simulation_to_Scaled_City-Zero-Shot_Policy_Transfer_for_Traffic_Control_via_Autonomous_Vehicles_Jang_UCBerkeley_2019.pdf)

 related paper: [Zero-Shot Learning and its Applications from Autonomous Vehicles to COVID-19 Diagnosis: A Review, Rezaei et al, U Leeds, 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Zero-Shot_Learning_and_its_Applications_from_Autonomous_Vehicles_to_COVID-19_Diagnosis-A_Review_Rezael_ULeeds_2020.pdf)

[Conditional Variational Autoencoders with Learnable Conditional Embeddings with Tim Rose](https://towardsdatascience.com/conditional-variational-autoencoders-with-learnable-conditional-embeddings-e22ee5359a2a)

[Differential Equations as a Pytorch Neural Network Layer with Kevin Hannay](https://towardsdatascience.com/differential-equations-as-a-pytorch-neural-network-layer-7614ba6d587f)

[ULTRA: Foundation Models for Knowledge Graph Reasoning with Michail Galkin](https://towardsdatascience.com/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09)

related paper: [A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs, X. Huang et al, Oxford U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Theory_of_Link_Prediction_via_Relational_Weisfeiler-Leman_on_Knowledge_Graphs_Huang_Oxford_2023.pdf)

related paper: [Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types, J. Gao et al, Purdue U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Double_Equivariance_for_Inductive_Link_Prediction_for_Both_New_Nodes_and_New_Relation_Types_Gao_PurdueU_2023.pdf)

related paper: [Edge Directionality Improves Learning on Heterophilic Graphs, Rossi et al, Imperial College, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Edge_Directionality_Improves_Learning_on_Heterophilic_Graphs_Rossi_2023.pdf)

related paper: [How Powerful are Graph Neural Networks? K. Xu et al, MIT, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/How_Powerful_are_Graph_Neural_Networks_Xu_MIT_2019.pdf)

related paper: [On The Equivalence Between Positional Node Embeddings and Structural Graph Representation, Srinivasan et al, Purdue U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/On_the_Equivalence_between_Positional_Node_Embeddings_and_Structural_Graph_Representations_Srinivasa_ISLR_2020.pdf)

related paper: [The Graph Neural Network Model, F. Scarcelli et al, U of Sienna, 2009](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TheGraphNeuralNetworkModelScarselli2009.pdf)

related paper: [Towards Foundation Models for Knowledge Graph Reasoning, M. Galkin et al, Intel AI Lab, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Towards_Foundation_Models_for_Knowledge_Graph_Reasoning_Galkin_2023.pdf)

related paper: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks, C Morris et al, TU Dortmund, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Weisfeiler_and_Leman_Go_Neural-Higher-order_Graph_Neural_Networks_Morris_2018.pdf)

related paper: [Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction, Zhu et al, Quebec AI Inst., 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Neural_Bellman-Ford_Networks-A_General_Graph_Neural_Network_Framework_for_Link_Prediction_Quebec_AI_Inst_Zhu_2022.pdf)

related paper: [NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs, M. Galkin et al, McGill, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/NodePiece-Compositional_and_Parameter-Efficient_Representations_of_Large_Knowledge_Graphs_Galkin_McGill_2022.pdf)

[A gentle introduction to Steerable Neural Networks (part 1) with Matteo Ciprian](https://towardsdatascience.com/a-gentle-introduction-to-steerable-neural-networks-part-1-32323d95b03f)

related paper: [3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data, M. Weller et al, U Amsterdam 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/3D_Steerable_CNNs-Learning_Rotationally_Equivariant_Features_in_Volumetric_Data_Weller_UAmsterdam_2018.pdf)

related paper: [Steerable CNNs, Taco S Cohen, Max Welling, U. Amsterdam, 2016](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Steerable_CNNs_Cohen_UAmsterdam_2016.pdf)

related paper: [Learning Steerable Filters for Rotation Equivariant CNNs, Maurice Weiler et al, U Amsterdam, 2018](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_Steerable_Filters_for_Rotation_Equivariant_CNNs_Weiler_UAmsterdam_2018.pdf)

related paper: [General E(2)-Equivariant Steerable CNNs, Maurice Weiler, Gabriele Cesa, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/General_E2-Equivariant_Steerable_CNNs_Weiler_2018.pdf)

related paper: [Scale Steerable Filters for the Locally Scale-Invariant Convolutional Neural Network, R. Ghosh et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Scale_Steerable_Filters_for_the_Locally_Scale-Invariant_Convolutional_Neural_Network_Ghosh_2019.pdf)

related paper: [A program to build E(n)-equivariant steerable CNNs, Cesa et al, U Amsterdam, Qualcomm AI Research, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_program_to_build_E_n-equivariant_steerable_CNNs_Cesa_2022.pdf)

[TiDE: the ‘embarrassingly’ simple MLP that beats Transformers with Rafael Guedes](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079)

related paper: [Long-term Forecasting with TiDE: Time-series Dense Encoder, Das et al, Google Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Long-term_Forecasting_with_TiDE-Time-series_Dense_Encoder_Das_GoogleResearch_2023.pdf)

related paper: [Darts: User-Friendly Modern Machine Learning for Time Series, Herzen et al, 2022](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Darts-User-Friendly_Modern_Machine_Learning_for_Time_Series_Herzen_2022.pdf)

related paper: [GluonTS: Probabilistic Time Series Models in Python, A. Alexandrov et al, 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/GluonTS-Probabilistic_Time_Series_Models_in_Python_Alexandrov_AWS_2019.pdf)

related paper: [Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting, B Lim et al, Oxford U., 2019](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Temporal_Fusion_Transformers_for_Interpretable_Multi-horizon_Time_Series_Forecasting_Lim_OxfordU_2020.pdf)

related paper: [An All-MLP Architecture for Time Series Forecasting, S. Chen et al, Google Cloud AI Research, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/TSMixer-An_All-MLP_Architecture_for_Time_Series_Forecasting_Chen_GoogleCloudAIResearch_2023.pdf)

### Physics based Deep Learning

[Unraveling the Design Pattern of Physics-Informed Neural Networks: Series 01 with Shuai Guo](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)

[Operator Learning via Physics-Informed DeepONet: Let’s Implement It From Scratch with Shuai Guo](https://towardsdatascience.com/operator-learning-via-physics-informed-deeponet-lets-implement-it-from-scratch-6659f3179887)

   related paper: [DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators, L. Lu et al, Brown U., 2020](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/DeepONet-Learning_nonlinear_operators_for_identifying_differential_equations_based_on_the_universal_approximation_theorem_of_operators_Lu_BrownU_2020.pdf)

   related paper: [Learning the Solution Operator of Paramteric Partial Differential Equations with Physics-Iinformed DeepONets, S. Wang, U Penn, 2021](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Learning_the_Solution_Operator_of_Parametric_Partial_Differential_Equations_with_Physics-Informed_DeepONets_Wang_UPenn_2021.pdf)

[Discovering Differential Equations with Physics-Informed Neural Networks and Symbolic Regression with Shuai Guo](https://towardsdatascience.com/discovering-differential-equations-with-physics-informed-neural-networks-and-symbolic-regression-c28d279c0b4d)

   related paper: [Interpretable Machine Learning for Science
with PySR and SymbolicRegression.jl, M. Carnmer, Princeton U., 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Interpretable_Machine_Learning_for_Science_with_PySR_and_SymbolicRegression.jl_Carnmer_2023.pdf)

   related paper: [Discovering a reaction-diffusion model for Alzheimer’s disease by
combining PINNs with symbolic regression, Z. Zhang, et al, Brown U, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Discovering_a_reaction-diffusion_model_for_Alzheimers_disease_by_combining_PINNs_with_symbolic_regression_Zhang_StanfordU_2023.pdf)

   related repo: https://github.com/ShuaiGuo16/PINN_symbolic_regression

[Solving Inverse Problems With Physics-Informed DeepONet: A Practical Guide With Code Implementation with Shuai Guo](https://towardsdatascience.com/solving-inverse-problems-with-physics-informed-deeponet-a-practical-guide-with-code-implementation-27795eb4f502)

### ML Ops for deep learning 
[Instance Selection for Deep Learning with Chaim Rand](https://towardsdatascience.com/instance-selection-for-deep-learning-7463d774cff0)

[Effective Load Balancing with Ray on Amazon SageMaker with Chaim Rand](https://towardsdatascience.com/effective-load-balancing-with-ray-on-amazon-sagemaker-d3b9020679d3)

[PyTorch Model Performance Analysis and Optimization (Part 1) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869)

[PyTorch Model Performance Analysis and Optimization (Part 2) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-2-3bc241be91)

[PyTorch Model Performance Analysis and Optimization (Part 3) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-3-1c5876d78fe2)

[PyTorch Model Performance Analysis and Optimization (Part 4) with Chaim Rand](https://towardsdatascience.com/solving-bottlenecks-on-the-data-input-pipeline-with-pytorch-profiler-and-tensorboard-5dced134dbe9)

[PyTorch Model Performance Analysis and Optimization (Part 5) with Chaim Rand](https://towardsdatascience.com/how-to-optimize-your-dl-data-input-pipeline-with-a-custom-pytorch-operator-7f8ea2da5206)

[PyTorch Model Performance Analysis and Optimization (Part 6) with Chaim Rand](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-part-6-b87412a0371b)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 1) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 2) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-2-c833e7139b5f)

[Smart Distributed Training on Amazon SageMaker with SMD (Part 3) with Chaim Rand](https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-3-db707db8a202)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 1) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-1-7877cdd6e879)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 2) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-2-23b1dac5ebed)

[How to Run Machine Learning Hyperparameter Optimization in the Cloud (Part 3) with Chaim Rand](https://towardsdatascience.com/how-to-run-machine-learning-hyperparameter-optimization-in-the-cloud-part-3-f66dddbe1415)

[Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation), Lucas de Lima Nogueira, 2024, Towards Data Science](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc)

## Vector Databases

[Foundations of Vector Retrieval, Sebastian Bruch, 2024, arXiv](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/books/Foundations_of_Vector_Retrieval_Bruch_2024.pdf)

[A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge, Y. Han et al, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/A_Comprehensive_Survey_on_Vector_Database-Storage_and_Retrieval_Technique_Challenge_Han_2023.pdf)

[Vector database management systems: Fundamental concepts, use-cases, and current challenges, T. Taipalus, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Vector_database_management_systems-Fundamental_concepts_use-cases_and_current_challenges_Taipalus_2023.pdf)

[Embeddings, Vector Databases and Search, Module 2 slides from Databricks course, 2023](https://github.com/dimitarpg13/deep_learning_and_neural_networks/blob/main/literature/articles/Embedding_Vector_DBs_and_Search_Databricks_slides_2023.pdf)

## Yan LeCun's Deep Learning Course at Center for Data Science, NYU

https://cds.nyu.edu/deep-learning/

[Transforming Deep Learning Education with Yann LeCun and Alfredo Canziani’s Free Online Course, NYU Center for Data Science, Medium](https://nyudatascience.medium.com/transforming-deep-learning-education-with-yann-lecun-and-alfredo-canzianis-free-online-course-6cccfd1970b3)

## online videos

### Neural Networks - from zero to hero, Andrej Karpathy, Feb 2023

[The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)

[The spelled-out intro to language modeling: building makemore](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 2: MLP](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc)

[Building makemore Part 4: Becoming a Backprop Ninja](https://youtu.be/q8SA3rM6ckI)

[Building makemore Part 5: Building a WaveNet](https://youtu.be/t3YJ5hKiMQ0)

## Relevant Repos

* [Intro to Deep Learning (Tebs Labs fork)](https://github.com/dimitarpg13/intro-to-deep-learning/tree/master) 

